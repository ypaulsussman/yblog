---
title: How We Learn (Carey, Benedict)
date: "2019-05-07"
template: "post"
draft: true
slug: "/posts/how-we-learn-notes/"
category: "What I Read"
tags:
  - "Neuroscience"
  - "Learning"
description: "Notes from a book I read. (Tagline: \"a practical, playful, and endlessly fascinating guide to what we really know about learning and memory today—and how we can apply it to our own lives.\")"
---

## Introduction: Broaden the Margins

we work more effectively, scientists have found, when we continually alter our study routines and abandon any “dedicated space” in favor of varied locations. Sticking to one learning ritual, in other words, slows us down.

Studies find that the brain picks up patterns more efficiently when presented with a mixed bag of related tasks than when it’s force-fed just one, no matter the age of the student or the subject area, whether Italian

Another common assumption is that the best way to master a particular skill—say, long division or playing a musical scale—is by devoting a block of time to repetitively practicing just that. Wrong again. Studies find that the brain picks up patterns more efficiently when presented with a mixed bag of related tasks than when it’s force-fed just one, no matter the age of the student or the subject area, whether Italian phrases or chemical bonds.

integrating learning into the more random demands of life can improve recall in many circumstances

the field is producing a swarm of new ideas that continue to complicate the picture. Dyslexia improves pattern recognition. Bilingual kids are better learners. Math anxiety is a brain disorder. Games are the best learning tool. Music training enhances science aptitude. But much of this is background noise, a rustling of the leaves. The aim in this book is to trace the trunk of the tree, the basic theory and findings that have stood up to scrutiny

## The Story Maker: The Biology of Memory

the entorhinal cortex, which acts as a kind of filter for incoming information; the hippocampus, where memory formation begins; and the neocortex, where conscious memories are stored once they’re flagged as keepers.

The cells that link to form these networks are called neurons. A neuron is essentially a biological switch. It receives signals from one side and—when it “flips” or fires—sends a signal out the other, to the neurons to which it’s linked. The neuron network that forms a specific memory is not a random collection. It includes many of the same cells that flared when a specific memory was first formed

It’s as if these cells are bound in collective witness of that experience. The connections between the cells, called synapses, thicken with repeated use, facilitating faster transmission of signals.

Brain scientists began to discover, first, that developing nerve cells—baby neurons, so to speak—are coded to congregate in specific locations in the brain, as if preassigned a job. “You’re a visual cell, go to the back of the brain.” “You, over there, you’re a motor neuron, go straight to the motor area.” This discovery undermined

Brain scientists began to discover, first, that developing nerve cells—baby neurons, so to speak—are coded to congregate in specific locations in the brain, as if preassigned a job. “You’re a visual cell, go to the back of the brain.” “You, over there, you’re a motor neuron, go straight to the motor area.” This discovery undermined the “interchangeable parts” hypothesis.

Milner’s work showed that there were at least two systems in the brain to handle memory, one conscious and the other subconscious. We can track and write down what we learned today in history class, or in geometry, but not in soccer practice or gymnastics, not in anything like the same way. Those kinds of physical skills accumulate without our having to think much about them.

The theory that memory was uniformly distributed, then, was wrong. The brain had specific areas that handled different types of memory formation.

Without a functioning hippocampus, people cannot form new, conscious memories.

memories, once formed, must therefore reside elsewhere, outside the hippocampus. The only viable candidate, scientists knew, was the brain’s thin outer layer, the neocortex. The neocortex is the seat of human consciousness, an intricate quilt of tissue in which each patch has a specialized purpose.

Visual patches are in the back. Motor control areas are on the side, near the ears. One patch on the left side helps interpret language; another nearby handles spoken language, as well as written.

This layer—the “top” of the brain, as it were—is the only area with the tools capable of re-creating the rich sensory texture of an autobiographical memory, or the assortment of factual associations for the word “Ohio” or the number 12.

To the extent that it’s possible to locate a memory in the brain, that’s where it resides: in neighborhoods along the neocortex primarily, not at any single address.

That the brain can find this thing and bring it to life so fast—instantaneously, for most of us, complete with emotion, and layers of detail—defies easy explanation. No one knows how that happens. And it’s this instant access that creates what to me is the brain’s grandest illusion: that memories are “filed away” like video scenes that can be opened with a neural click, and snapped closed again.

to grasp what people do to retrieve a memory—to remember—requires stepping back for a wide shot.

Why don’t we feel two-brained, if we have these two copilots?

“Why, if we have these separate systems, is it that the brain has a sense of unity?”

The left brain/right brain differences revealed a clear, and fascinating, division of labor. Yet scientists kept finding other, more intricate, divisions. The brain has thousands, perhaps millions, of specialized modules, each performing a special skill—one calculates a change in light, for instance, another parses a voice tone, a third detects changes in facial expression. The more experiments that scientists did, the more specializing they found, and all of these mini-programs run at the same time, often across both hemispheres. That is, the brain sustains a sense of unity not only in the presence of its left and right copilots. It does so amid a cacophony of competing voices coming from all quarters, the neural equivalent of open outcry

The left hemisphere takes whatever information it gets and tells a tale to conscious awareness.

The brain’s cacophony of voices feels coherent because some module or network is providing a running narration.

All we know about this module is it resides somewhere in the left hemisphere. No one has any idea how it works, or how it strings together so much information so fast. It does have a name. Gazzaniga decided to call our left brain narrating system “the interpreter.”

This module is vital to forming a memory in the first place. It’s busy answering the question “What just happened?” in the moment, and those judgments are encoded through the hippocampus. That’s only part of the job, however.

the brain absorbs a lot more information in the moment than we’re consciously aware of, and those perceptions can surface during remembering. That is to say: The brain does not store facts, ideas, and experiences like a computer does, as a file that is clicked open, always displaying the identical image. It embeds them in networks of perceptions, facts, and thoughts, slightly different combinations of which bubble up each time. And that just retrieved memory does not overwrite the previous one but intertwines and overlaps with it. Nothing is completely lost, but the memory trace is altered and for good.

As scientists put it, using our memories changes our memories.

## The Power of Forgetting: A New Theory of Learning

common, self-defeating assumption: To forget is to fail. This appears self-evident.

If learning is building up skills and knowledge, then forgetting is losing some of what was gained. It seems like the enemy of learning. It’s not. The truth is nearly the opposite.

there are large upsides to forgetting,

it is nature’s most sophisticated spam filter. It’s what allows the brain to focus, enabling sought-after facts to pop to mind.

If recollecting is just that—a re-collection of perceptions, facts, and ideas scattered in intertwining neural networks in the dark storm of the brain—then forgetting acts to block the background noise, the static, so that the right signals stand out. The sharpness of the one depends on the strength of the other.

Normal forgetting—that passive decay we so often bemoan—is also helpful for subsequent learning.

the muscle-building property of forgetting: Some “breakdown” must occur for us to strengthen learning when we revisit the material. Without a little forgetting, you get no benefit from further study. It is what allows learning to build, like an exercised muscle.

no complex memory comes back exactly the same way twice, in part because the forgetting filter blocks some relevant details along with many irrelevant ones. Features that previously were blocked or forgotten often reemerge.

retrieving any memory alters its accessibility, and often its content. There is an emerging theory that accounts for these and related ideas. It’s called the New Theory of Disuse, to distinguish it from an older, outdated principle stating, simply, that memories evaporate entirely from the brain over time if they’re not used. The new theory is far more than an updating, though. It’s an overhaul, recasting forgetting as the best friend of learning, rather than its rival.

A better name for it, then, might be the Forget to Learn theory.

The Forgetting Curve is exactly what it sounds like, a graph of memory loss over time. In particular, it charts the rate at which newly learned information fades from memory.

In 1914, the influential American education researcher Edward Thorndike turned Ebbinghaus’s curve into a “law” of learning. He called it the Law of Disuse, which asserted that learned information, without continued use, decays from memory entirely—i.e., use it or lose it. The law felt right. It certainly seemed to square with experience, defining how most people thought of learning and to this day still do. Yet that definition hides more than it reveals.

the test that an English teacher and researcher named Philip Boswood Ballard began administering to schoolchildren in the early 1900s in London’s working-class East End. The children were thought to be poor learners, and Ballard was curious to find out why.

Ballard doubted what he was seeing and ran hundreds of additional tests, with more than ten thousand subjects, over the next several years. The results were the same: Memory improved in the first few days without any further study, and only began to

Ballard doubted what he was seeing and ran hundreds of additional tests, with more than ten thousand subjects, over the next several years. The results were the same: Memory improved in the first few days without any further study, and only began to taper off after day four or so, on average.

“We not only tend to forget what we have once remembered,” he wrote, “but we also tend to remember what we have once forgotten.” Memory does not have just one tendency over time, toward decay. It has two.

The other—“reminiscence,” Ballard called it—is a kind of growth, a bubbling up of facts or words that we don’t recall having learned in the first place.

Forgetting, remember, is not only a passive process of decay but also an active one, of filtering. It works to block distracting information, to clear away useless clutter.

It wasn’t long before many scientists followed Buxton’s lead and begged off the hunt.

one British learning theorist, C. E. Buxton, concluded that Ballard’s spontaneous improvement effect was a “now-you-see-it-now-you-don’t phenomenon”—in other words, a phantom. It wasn’t long before many scientists followed Buxton’s lead and begged off the hunt.

Besides, the real juice in learning science by the middle of the century was in reinforcement. It was the high summer of behaviorism.

The reason researchers had had so much trouble isolating Ballard’s “reminiscence” was because the strength of this effect is highly dependent on the material being used. For nonsense syllables, and for most lists of vocabulary words or random sentences, it’s zero: There’s no spontaneous improvement on test scores after a day or two. By contrast, reminiscence is strong for imagery,

Ballard had identified the “bubbling up” of new verse in the first few days after study, when it’s strongest. Other researchers had looked for it too early, minutes afterward, or too late, after a week or more.

Why does recall of pictures improve while recall of word lists does not?

Robert Bjork of UCLA and his wife, Elizabeth Ligon Bjork, also at UCLA. The new theory of disuse (“Forget to Learn,” as we’re calling it) is largely their baby.

Any memory has two strengths, a storage strength and a retrieval strength.

Storage strength is just that, a measure of how well learned something is. It builds up steadily with studying, and more sharply with use.

The multiplication table is a good example. It’s drilled into our heads in grade school, and we use it continually throughout life, in a wide variety of situations, from balancing the bank account to calculating tips to helping our fourth grader with homework. Its storage strength is enormous. According to the Bjorks’ theory, storage strength can increase but it never decreases.

everything we have deliberately committed to memory—the multiplication table, a childhood phone number, the combination to our first locker—is all there, and for good. This seems beyond belief at first, given the sheer volume of information we absorb and how mundane so much of it

is. Remember from chapter 1, though, that biologically speaking there’s space to burn: in digital terms, storage space for three million TV shows. That is more than enough to record every second of a long life, cradle to grave. Volume is not an issue.

no memory is ever “lost” in the sense that it’s faded away, that it’s gone. Rather, it is not currently accessible. Its retrieval strength is low, or near zero. Retrieval strength, on the other hand, is a measure of how easily a nugget of information comes to mind. It, too, increases with studying, and with use. Without reinforcement, however, retrieval strength drops off quickly, and its capacity is relatively small (compared to storage). At any given time, we can pull up only a limited number of items in connection with any given cue or reminder.

Compared to storage, retrieval strength is fickle. It can build quickly but also weaken quickly.

The act of finding and naming each person increases both strengths, remember. The first grade teacher—once she’s reintroduced—is now highly retrievable. This is due to the passive side of forgetting, the fading of retrieval strength over time. The theory says that that drop facilitates deeper learning once the fact or memory is found again.

That’s not all. The harder we have to work to retrieve a memory, the greater the subsequent spike in retrieval and storage strength (learning). The Bjorks call this principle desirable difficulty,

The brain developed this system for a good reason, the Bjorks argue. In its nomadic hominid youth, the brain was continually refreshing its mental map to adapt to changing weather, terrain, and predators. Retrieval strength evolved to update information quickly, keeping the most relevant details handy. It lives for the day. Storage strength, on the other hand, evolved so that old tricks could be relearned, and fast, if needed. Seasons pass, but they repeat; so do weather and terrain. Storage strength plans for the future. This combination of flighty retrieval and steady storage—the tortoise and the hare—is no less important to modern-day survival

“Compared to some kind of system in which out-of-date memories were to be overwritten or erased,” Bjork writes, “having such memories become inaccessible but remain in storage has important advantages. Because those memories are inaccessible, they don’t interfere with current information and procedures. But because they remain in memory they can—at least under certain circumstances—be relearned.”

Using memory changes memory—and for the better. Forgetting enables and deepens learning, by filtering out distracting information and by allowing some breakdown that, after reuse, drives retrieval and storage strength higher than they were originally.

## Breaking Good Habits: The Effect of Context on Learning

The higher test scores square with reinstatement theory: The background music weaves itself subconsciously into the fabric of stored memory. Cue up the same music, and more of those words are likely to resurface.

The lower scores in the quiet room (after quiet study) are harder to explain. Smith argued that they may be due to an absence of cues to reinstate. The students “do not encode the absence of sound any more than they might encode the absence of any type of stimulus, such as pain or food,” he wrote. As a result the study environment is impoverished, compared to one with music in the background.

Having something going on in the study environment, like music, is better than nothing (so much for sanctity of the quiet study room).

the experience of studying has more dimensions than we notice, some of which can have an impact on retention. The contextual cues scientists describe—music, light, background colors—are annoyingly ephemeral, it’s true. They’re subconscious, usually untraceable. Nonetheless, it is possible to recognize them at work in our own lives.

The science tells us that, at least when it comes to retention of new facts, the subconscious ones are valuable, too. Not always—when we’re submerged in analytical work, they’re negligible—and not necessarily all of them.

Give someone a real hint—like a category name—and it easily trumps the internal cues.

We can easily multiply the number of perceptions connected to a given memory—most simply, by varying where we study.

Daniel Willingham, a leading authority on the application of learning techniques in classrooms,

advises his own students, when they’re reviewing for an exam, not to work straight from their notes. “I tell them to put the notes aside and create an entirely new outline, reorganizing the material,” he told me. “It forces you to think about the material again, and in a different way.”

## Spacing Out: The Advantage of Breaking Up Study Time

distributed learning or, more commonly, the spacing effect. People learn at least as much, and retain it much longer, when they distribute—or “space”—their study time than when they concentrate it.

Not just better, a lot better. Distributed learning, in certain situations, can double the amount we remember later on.

for much of the last hundred years psychologists have—exasperatingly, inexplicably—confined the study of spacing to short lab experiments.

Only in the last several years have researchers mapped out the best intervals to use when spacing study time. Is it more efficient to study a little bit today and a little bit tomorrow, or to do so every other day, or once a week? What if it’s Tuesday, and the history final is on Friday? What if the exam is a month away? Do the spacing intervals change depending on the exam date? I see the history of distributed learning as an object lesson in how to interpret research,

Jost’s Law: “If two associations are of equal strength but of different age, a new repetition has a greater value for the older one.” Translation: Studying a new concept right after you learn it doesn’t deepen the memory much, if at all; studying it an hour later, or a day later, does.

Why spaced study sessions have such a large impact on learning is still a matter of debate. Several factors are likely at work, depending on the interval. With very short intervals—seconds or minutes, as in the early studies—it may be that the brain becomes progressively less interested in a fact when it’s repeated multiple times in rapid succession.

For intermediate intervals of days or weeks, other factors might come into play. Recall the Forget to Learn theory, which holds that forgetting aids learning in two ways: actively, by filtering out competing facts, and passively, in that some forgetting allows subsequent practice to deepen learning, like an exercised muscle.

meeting the new neighbors for the first time (“Justin and Maria, what great names”). You remember the names right after hearing them, as retrieval strength is high. Yet storage strength is low, and by tomorrow morning the names will be on the tip of your tongue. Until you hear, from over the hedges—“Justin! Maria!”—and you got ’em, at least for the next several days. That is to say: Hearing the names again triggers a mental act, retrieval—Oh that’s right, Justin as in Timberlake and Maria as in Sharapova—which boosts subsequent retrieval strength higher than it previously was. A day has passed between workouts, allowing strength to increase.

Spaced study—in many circumstances, including the neighbor example—also adds contextual cues,

You initially learned the names at the party, surrounded by friends and chatter, a glass of wine in hand. The second time, you heard them yelled out, over the hedges.

The effects described above are largely subconscious, running under the radar. We don’t notice them. With longer intervals of a month or more, and especially with three or more sessions, we begin to notice some of the advantages that spacing allows, because they’re obvious.

“With longer spaces, you’re forgetting more, but you find out what your weaknesses are and you correct for them,” Bahrick told me. “You find out which mediators—which cues, which associations, or hints you used for each word—are working and which aren’t. And if they’re not working, you come up with new ones.”

- Your Highlight on page 78 | Location 1207 | Added on Thursday, May 2, 2019 5:12:42 PM

If the test is in a week, and you want to split your study time in two, then do a session today and tomorrow, or today and the day after tomorrow. If you want to add a third, study the day before the test (just under a week later). If the test is a month away, then the best option is today, a week from today (for two sessions); for a third, wait three more weeks or so, until a day before the test. The further away the exam—that is, the more the time you have to prepare—the larger the optimal interval between sessions one and two. That optimal first interval declines as a proportion of the time-to-test, the Internet study found. If the test is in a week, the best interval is a day or two (20 to 40 percent). If it’s in six months, the best interval is three to five weeks (10 to 20 percent). Wait any longer between study sessions, and performance goes down fairly quickly.

Remember, spacing is primarily a retention technique. Foreign languages. Science vocabulary. Names, places, dates, geography, memorizing speeches. Having more facts on board could very well help with comprehension, too, and several researchers are investigating just that, for math as well as other sciences. For now, though, this is a memorization strategy.

## The Hidden Value of Ignorance: The Many Dimensions of Testing

what psychologists call fluency, the belief that because facts or formulas or arguments are easy to remember right now, they’ll remain that way tomorrow or the next day. The fluency illusion is so strong that, once we feel we’ve nailed some topic or assignment, we assume that further study won’t help. We forget that we forget.

Fluency misperceptions are automatic. They form subconsciously and make us poor judges of what we need to restudy, or practice again. “We know that if you study something twice, in spaced sessions, it’s harder to process the material the second time, and so people think it’s counterproductive,” as Nate Kornell, a psychologist at Williams College, told me. “But the opposite is true: You learn more, even though it feels harder. Fluency is playing a trick on judgment.”

recall the Bjorks’ “desirable difficulty” principle: The harder your brain has to work to dig out a memory, the greater the increase in learning (retrieval and storage strength). Fluency, then, is the flipside of that equation. The easier it is to call a fact to mind, the smaller the increase in learning. Repeating facts right after you’ve studied them gives you nothing, no added memory benefit. The fluency illusion is the primary culprit in below-average test performances.

The best way to overcome this illusion and improve our testing skills is, conveniently, an effective study technique in its own right.

The technique is testing itself. Yes, I am aware of how circular this logic appears: better testing through testing. Don’t be fooled. There’s more to self-examination than you know. A test is not only a measurement tool, it alters what we remember and changes how we subsequently organize that knowledge in our minds. And it does so in ways that greatly improve later performance.

Arthur Gates was interested in, among other things, how the act of recitation interacts with memory. For centuries, students who received a classical education spent untold hours learning to recite from memory epic poems, historic monologues, and passages from scripture—a skill that’s virtually lost today. Gates wanted to know whether there was an ideal ratio between reading (memorizing) and reciting (rehearsal).

“In general,” he concluded, “the best results are obtained by introducing recitation after devoting about 40 percent of the time to reading. Introducing recitation too early or too late leads to poorer results,” Gates wrote. In the older grades, the percentage was even smaller, closer to a third. “The superiority of optimal reading and retention over reading alone is about 30 percent.”

The quickest way to download that St. Crispin’s Day speech, in other words, is to spend the first third of your time memorizing it, and the remaining two thirds reciting

Herbert F. Spitzer was a doctoral student at the State University of Iowa, who in 1938 was trawling for a dissertation project.

one of the biggest questions hanging over teachers, from the very beginning of the profession, was when testing is most effective. Is it best to give one big exam at the end of a course? Or do periodic tests given earlier in the term make more sense?

The groups that took pop quizzes soon after reading the passage—once or twice within the first week—did the best on a final exam given at the end of two months,

getting about 50 percent of the questions correct. (Remember, they’d studied their peanut or bamboo article only once.) By contrast, the groups who took their first pop quiz two weeks or more after studying scored much lower, below 30 percent on the final. Spitzer showed not only that testing is a powerful study technique, he showed it’s one that should be deployed sooner rather than later.

Henry Roediger III and Jeffrey Karpicke, also then at Washington University, in a landmark 2006 review of the testing effect, as they called it.

the “testing” prep buried the “study” prep when it really mattered, on the one-week test. In short, testing does not = studying, after all. In fact, testing > studying, and by a country mile, on delayed tests.

Roediger, who’s contributed an enormous body of work to learning science, both in experiments and theory, also happens to be one of the field’s working historians. In a review paper published in 2006, he and Karpicke analyzed a century’s worth of experiments, on all types of retention strategies (like spacing, repeated study, and context), and showed that the testing effect has been there all along, a strong, consistent “contaminant,” slowing down forgetting. To measure any type of learning, after all, you have to administer a test. Yet if you’re using the test only for measurement, like some physical education push-up contest, you fail to see it as an added workout—itself making contestants’ memory muscles stronger.

The word “testing” is loaded, in ways that have nothing to do with learning science.

In part to soften this resistance, researchers have begun to call testing “retrieval practice.” That phrase is a good one for theoretical reasons, too. If self-examination is more effective than straight studying (once we’re familiar with the material), there must be reasons for it. One follows directly from the Bjorks’ desirable difficulty principle. When the brain is retrieving studied text, names, formulas, skills, or anything else, it’s doing something different, and harder, than when it sees the information again, or restudies. That extra effort deepens the resulting storage and retrieval strength. We know the facts or skills better because we retrieved them ourselves, we didn’t merely review them.

what if, instead, you took a test on Day 1 that was comprehensive but not a replica of the final exam? You’d bomb the thing, to be sure. You might not be able to understand a single question. And yet that experience, given what we’ve just learned about testing, might alter how you subsequently tune into the course itself during the rest of the term. This is the idea behind pretesting, the latest permutation of the testing effect.

in some circumstances, unsuccessful retrieval attempts—i.e., wrong answers—aren’t merely random failures. Rather, the attempts themselves alter how we think about, and store, the information contained in the questions. On some kinds of tests, particularly multiple-choice, we learn from answering incorrectly—especially when given the correct answer soon afterward.

That is, guessing wrongly increases a person’s likelihood of nailing that question, or a related one, on a later test.

If you’re like most people, you scored 10 to 20 percent higher on the countries in that first group, the ones where you guessed before hearing the correct answer. In the jargon of the field, your “unsuccessful retrieval attempts potentiated learning, increasing successful retrieval attempts on subsequent tests.” In plain English: The act of guessing engaged your mind in a different and more demanding way than straight memorization did, deepening the imprint of the correct answers. In even plainer English, the pretest drove home the information in a way that studying-as-usual did not. Why? No one knows for sure. One possible explanation is that pretesting is another manifestation of desirable difficulty.

You work a little harder by guessing first than by studying directly. A second possibility is that the wrong guesses eliminate the fluency illusion, the false impression that you knew the capital of Eritrea because you just saw or studied it. A third is that, in simply memorizing, you saw only the correct answer and weren’t thrown off by the other four alternatives—the way you would be on a test.

Even when students bomb a test, she said, they get an opportunity to see the vocabulary used in the coming lectures and get a sense of what kinds of questions and distinctions between concepts are important.

Pretesting is not an entirely new concept. We have all taken practice tests at one time or another as a way of building familiarity

Pretesting is not an entirely new concept. We have all taken practice tests at one time or another as a way of building familiarity

the practice runs are primarily about reducing anxiety and giving us a feel for format and timing. The research that the Bjorks, Roediger, Kornell, Karpicke and others have done is different. Their testing effect—pre- or post-study—applies to learning the kind of concepts, terms, and vocabulary that form a specialized knowledge base,

“At this point, we don’t know what the ideal applications of pretesting are,” Robert Bjork told me. “It’s still a very new area.”

Remember: These apparently simple attempts to communicate what you’ve learned, to yourself or others, are not merely a form of self-testing, in the conventional sense, but studying—the high-octane kind, 20 to 30 percent more powerful than if you continued sitting on your butt, staring at that outline. Better yet, those exercises will dispel the fluency illusion. They’ll expose what you don’t know, where you’re confused, what you’ve forgotten—and fast.

These apparently simple attempts to communicate what you’ve learned, to yourself or others, are not merely a form of self-testing, in the conventional sense, but studying—the high-octane kind, 20 to 30 percent more powerful than if you continued sitting on your butt, staring at that outline. Better yet, those exercises will dispel the fluency illusion. They’ll expose what you don’t know, where you’re confused, what you’ve forgotten

## Quitting Before You’re Ahead: The Accumulating Gifts of Percolation

the first two elements of percolation: interruption, and the tuned, scavenging mind that follows. The journal entries provided the third element, conscious reflection. Remember, Dively had the students make regular entries on what they thought about the sources they used, the journal articles and interviews. Their thinking evolved, entry by entry, as they accumulated more knowledge. Assembled into a coherent whole, this research—from Zeigarnik, Aarts, Dively, and other social psychologists who’ve spent the past decades studying goal fulfillment—takes some of the mystery out of the “creative process.” No angel or muse is whispering to anyone here. Percolation is a matter of vigilance, of finding ways to tune the mind so that it collects a mix of external perceptions and internal thoughts that are relevant to the project at hand. We can’t know in advance what those perceptions and thoughts will look like—and we don’t have to. Like the thirsty students in Aarts’s study, the information flows in. If more

It suggests that we should start work on large projects as soon as possible and stop when we get stuck, with the confidence that we are initiating percolation, not quitting.

Quitting before I’m ahead doesn’t put the project to sleep; it keeps it awake. That’s Phase 1, and it initiates Phase 2, the period of gathering string, of casual data collecting. Phase 3 is listening to what I think about all those incoming bits and pieces. Percolation depends on all three elements, and in that order.

## Being Mixed Up: Interleaving as an Aid to Comprehension

Kerr and Booth. “A varied practice schedule may facilitate the initial formation of motor schema,” they wrote, the variation working to “enhance movement awareness.” In other words: Varied practice is more effective than the focused kind, because it forces us to internalize general rules of motor adjustment that apply to any hittable target.

Kinetics and cognitive psychology are worlds apart in culture and in status. One is closer to brain science, the other to gym class.

Psychologists who study learning tend to fall into one of two camps: the motor/movement, or the verbal/academic. The former focuses on how the brain sees, hears, feels, develops reflexes, and acquires more advanced physical abilities, like playing sports or an instrument. The latter investigates conceptual learning of various kinds: language, abstract ideas, and problem solving. Each camp has its own vocabulary, its own experimental paradigms, its own set of theories. In college, they are often taught separately, in different courses: “Motor and Perceptual Skills” and “Cognition and Memory.” This distinction is not an arbitrary one.

A major implication of the Molaison studies was that the brain must have at least two biological systems for handling memory. One, for declarative memories, is dependent on a functioning hippocampus. The other, for motor memories, is based in different brain organs; no hippocampus required. The two systems are biologically distinct, so it stood to reason that they’re functionally distinct, too, in how they develop, strengthen, and fade. Picking up Spanish is not the same as picking up Spanish guitar, and so psychology has a separate tradition to characterize each.

Goode and Magill wanted not only to compare the relative effectiveness of each type of practice schedule. They also wanted to measure how well the participants’ skills transferred to a new condition. Transfer is what learning is all about, really. It’s the ability to extract the essence of a skill or a formula or word problem and apply it in another context, to another problem that may not look the same, at least superficially. If you’ve truly mastered a skill, you “carry it with you,” so to speak. Goode and Magill measured transfer in a subtle, clever way. On their final test of skill, they made one small adjustment: The participants served from the left side of the court, even though they’d

practiced only on the right.

Interfering with concentrated or repetitive practice forces people to make continual adjustments, they reasoned, building a general dexterity that, in turn, sharpens each specific skill.

Goode and Magill then took it one step further. All that adjusting during a mixed-practice session, they wrote, also enhances transfer. Not only is each skill sharper; it’s performed well regardless of context, whether indoors or out, from the right side of the court or the left.

Goode and Magill then took it one step further. All that adjusting during a mixed-practice session, they wrote, also enhances transfer. Not only is each skill sharper; it’s performed well regardless of context, whether indoors or out, from the right side of the court or the left.

Whenever researchers scrambled practice sessions, in one form or another, people improved more over time than if their practice was focused and uninterrupted

It’s not that repetitive practice is bad. We all need a certain amount of it to become familiar with any new skill or material. But repetition creates a powerful illusion. Skills improve quickly and then plateau. By contrast, varied practice produces a slower apparent rate of improvement in each single practice session but a greater accumulation of skill and learning over time. In the long term, repeated practice on one skill slows us down.

In their Forget to Learn theory, Robert and Elizabeth Bjork called any technique that causes forgetting a “desirable difficulty,” in that it forces the brain to work harder to dig up a memory or skill—and that added work intensifies subsequent retrieval and storage strength (learning).



Interleaving. That’s a cognitive science word, and it simply means mixing related but distinct material during study. Music teachers have long favored a variation on this technique, switching from scales, to theory, to pieces all in one sitting. So have coaches and athletic trainers, alternating endurance and strength exercises to ensure recovery periods for certain muscles. These philosophies are largely rooted in tradition, in a person’s individual experience, or in concerns about overuse. Kornell and Bjork’s painting study put interleaving on the map as a general principle of learning, one that could sharpen the imprint of virtually any studied material.

This much is clear: The mixing of items, skills, or concepts during practice, over the longer term, seems to help us not only see the distinctions between them but also to achieve a clearer grasp of each one individually. The hardest part is abandoning our primal faith in repetition.

Mixing problems during study forces us to identify each type of problem and match it to the appropriate kind of solution. We are not only discriminating between the locks to be cracked; we are connecting each lock with the right key. “The difficulty of pairing a problem with the appropriate procedure or concept is ubiquitous in mathematics,” Rohrer and Taylor concluded.

The evidence so far suggests that interleaving is likely applicable not just to math, but to almost any topic or skill.

the science suggests that interleaving is, essentially, about preparing the brain for the unexpected.

## Learning Without Thinking: Harnessing Perceptual Discrimination

In 1969, Eleanor Gibson published Principles of Perceptual Learning and Development, a book that brought together all her work and established a new branch of psychology: perceptual learning. Perceptual learning, she wrote, “is not a passive absorption, but an active process, in the sense that exploring and searching for perception itself is active. We do not just see, we look; we do not just hear, we listen. Perceptual learning is self-regulated, in the sense that modification occurs without the necessity of external reinforcement. It is stimulus oriented, with the goal of extracting and reducing the information simulation. Discovery of distinctive features and structure in the world is fundamental in the achievement of this goal.”

Only in the past decade or so have scientists begun to exploit Gibson’s findings

Computer PLMs as Kellman and others have designed them are visual, fast-paced, and focused on classifying images (do the elevated bumps in that rash show shingles, eczema, or psoriasis?) or problems rather than solving them outright

Computer PLMs as Kellman and others have designed them are visual, fast-paced, and focused on classifying images (do the elevated bumps in that rash show shingles, eczema, or psoriasis?) or problems rather than solving them outright (does that graph match x—3y = 8, or x + 12 y + 32?). The modules are intended to sharpen snap judgments—perceptual skills—so that you “know” what you’re looking at without having to explain why, at least not right away. In effect, the PLMs build perceptual intuition

Scientists have a lot more work to do before they figure out how, and for which subjects, PLMs are most effective.

It’s a supplement to experience, not a substitute. That’s one reason perceptual learning remains a backwater in psychology and education. It’s hardly a reason to ignore it, though. Perceptual learning is happening all the time, after all, and automatically—and it’s now clear that it can be exploited to speed up acquisition of specific skills.

PLMs are meant for a certain kind of target: discriminating or classifying things that look the same to the untrained eye but are not.
