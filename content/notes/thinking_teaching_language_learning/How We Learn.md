---
title: How We Learn (Carey, Benedict)
date: "2019-05-07"
template: "post"
draft: true
slug: "/posts/how-we-learn-notes/"
category: "What I Read"
tags:
  - "Neuroscience"
  - "Learning"
description: "Notes from a book I read. (Tagline: \"a practical, playful, and endlessly fascinating guide to what we really know about learning and memory today—and how we can apply it to our own lives.\")"
---

## Introduction: Broaden the Margins

We work more effectively, scientists have found, when we continually alter our study routines and abandon any “dedicated space” in favor of varied locations. Sticking to one learning ritual, in other words, slows us down.

A common (_and wrong_) assumption is that the best way to master a particular skill (_e.g. long division or a musical scale_) is by devoting a block of time to repetitively practicing just that. Studies instead find that the brain picks up patterns more efficiently when presented with a mixed bag of related tasks than when it’s force-fed just one, no matter the age of the student or the subject area.

Integrating learning into the more random demands of life can improve recall in many circumstances.

The field is producing a swarm of new ideas that continue to complicate the picture. 
- Dyslexia improves pattern recognition. 
- Bilingual kids are better learners. 
- Math anxiety is a brain disorder. 
- Games are the best learning tool. 
- Music training enhances science aptitude. 

But much of this is background noise, a rustling of the leaves. The aim in this book is to trace the trunk of the tree, the basic theory and findings that have stood up to scrutiny.

## The Story Maker: The Biology of Memory

The cells that link to form these networks are called _neurons_
  - A neuron is essentially a biological switch. 
  - It receives signals from one side and — when it “flips” or fires — sends a signal out the other, to the neurons to which it’s linked. 

The neuron network that forms a specific memory is not a random collection. 
  - It includes many of the same cells that flared when a specific memory was first formed.
  - It’s as if these cells are bound in collective witness of that experience. 
  - The connections between the cells, called _synapses_, thicken with repeated use, facilitating faster transmission of signals.

Brain scientists began to discover, first, that developing nerve cells — baby neurons, so to speak — are coded to congregate in specific locations in the brain, as if preassigned a job. 
  - i.e. “You’re a visual cell, go to the back of the brain.” “You, over there, you’re a motor neuron, go straight to the motor area.” 
  - This discovery undermined the “interchangeable parts” hypothesis.

Milner’s work showed that there were at least two systems in the brain to handle memory, one conscious and the other subconscious. 
- We can track and write down what we learned today in history class, or in geometry
- We can't in soccer practice, or gymnastics; those kinds of physical skills accumulate without our having to think much about them.

The theory that memory was uniformly distributed, then, was wrong. The brain had specific areas that handled different types of memory formation.

The hippocampus is where memory formation begins. Without a functioning hippocampus, people cannot form new, conscious memories.

Memories, once formed, reside elsewhere, outside the hippocampus. 
  - The brain’s thin outer layer, the neocortex, is the seat of human consciousness: an intricate quilt of tissue in which each patch has a specialized purpose.
    - Visual patches are in the back. 
    - Motor control areas are on the side, near the ears. 
    - One patch on the left side helps interpret language; another nearby handles spoken language, as well as written.
  - This layer — the “top” of the brain, as it were — is the only area with the tools capable of re-creating the rich sensory texture of an autobiographical memory, or the assortment of factual associations for the word “Ohio” or the number 12.
  - To the extent that it’s possible to locate a memory in the brain, that’s where it resides: in neighborhoods along the neocortex primarily, not at any single address.

That the brain can find a memory and bring it to life so fast defies easy explanation: no one knows how it happens. 

This instant access creates the illusion that memories are “filed away,” like video scenes that can be opened and snapped closed again.

The brain has thousands, perhaps millions, of specialized modules, each performing a special skill—one calculates a change in light, for instance, another parses a voice tone, a third detects changes in facial expression. 

All of these mini-programs run at the same time, often across both hemispheres. 

The brain sustains a sense of unity not only in the presence of its left and right copilots. It does so amid a cacophony of competing voices coming from all quarters, the neural equivalent of open outcry.

The brain’s cacophony of voices feels coherent because some module or network is providing a running narration. All we know about this module is it resides somewhere in the left hemisphere. No one has any idea how it works, or how it strings together so much information so fast.

This module is vital to forming a memory in the first place. It’s busy answering the question “What just happened?” in the moment, and those judgments are encoded through the hippocampus. That’s only part of the job, however.

The brain absorbs a lot more information in the moment than we’re consciously aware of, and those perceptions can surface during remembering.

The brain does not store facts, ideas, and experiences like a computer does, as a file that is clicked open, always displaying the identical image. 

It embeds them in networks of perceptions, facts, and thoughts, slightly different combinations of which bubble up each time. 

Such a just-retrieved memory does not overwrite the previous one but intertwines and overlaps with it.

As scientists put it, using our memories changes our memories.

## The Power of Forgetting: A New Theory of Learning

A common, self-defeating assumption: To forget is to fail. 
  - This appears self-evident: if learning is building up skills and knowledge, then forgetting is losing some of what was gained. 
  - It’s not. The truth is nearly the opposite.

Forgetting is not only a passive process of decay but also an active one, of filtering. It works to block distracting information, to clear away useless clutter.

Forgetting is nature’s most sophisticated spam filter: it allows the brain to focus, enabling sought-after facts to pop to mind.
  - Recollecting is just that: a re-collection of perceptions, facts, and ideas scattered in intertwining neural networks in the dark storm of the brain
  - Forgetting acts to block much of the background noise, the static, so that the right signals stand out. 
  - That is, the sharpness of the one depends on the strength of the other.

Old Theory of Disuse: memories evaporate entirely from the brain over time if they’re not used. 

Ebbinghaus’s Forgetting Curve is a graph of memory loss over time. It charts the rate at which newly learned information fades from memory.

In 1914, Edward Thorndike turned Ebbinghaus’s curve into a “law” of learning. 
  - He called it the Law of Disuse
  - It asserted that learned information, without continued use, decays from memory entirely — i.e., use it or lose it. 
  - That definition, however, hides more than it reveals.

An English teacher and researcher named Philip Boswood Ballard began administering memory tests to schoolchildren in the early 1900's 
  - Ballard doubted what he was seeing and ran hundreds of additional tests, with more than ten thousand subjects, over the next several years. 
  - The results were the same: memory improved in the first few days without any further study, and only began to taper off after day four or so, on average.
  - “We not only tend to forget what we have once remembered,” he wrote, “but we also tend to remember what we have once forgotten.” 
  - Ballard had identified the “bubbling up” of new verse in the first few days after study, when it’s strongest. 
    - Other researchers looked for it too early, minutes afterward, or too late, after a week or more.
    - One British learning theorist, C. E. Buxton, concluded that Ballard’s spontaneous improvement effect was a phantom; most scientists followed Buxton’s lead.
    - By the middle of the century, reinforcement and behaviorism were the main focus of learning science.
  - The major reason researchers had so much trouble replicating Ballard’s “reminiscence” was because the strength of this effect is highly dependent on the material being used. 
    - For nonsense syllables, and for most lists of vocabulary words or random sentences, it’s zero: There’s no spontaneous improvement on test scores after a day or two. 
    - By contrast, reminiscence is strong for imagery.

New Theory of Disuse: 
  - No complex memory comes back exactly the same way twice, in part because the forgetting filter blocks some relevant details along with many irrelevant ones. 
  - Features that previously were blocked or forgotten often reemerge.
  - Retrieving any memory, then, alters its accessibility, and often its content. 
  - Forgetting works as a muscle-building property: 
    - Some “breakdown” must occur for us to strengthen learning when we revisit the material. 
    - Without a little forgetting, you get no benefit from further study. It is what allows learning to build, like an exercised muscle.
  - As such, forgetting is a necessary component of learning, rather than its rival.
  - An alternative name for it is the Forget to Learn theory.
  - Robert Bjork and his wife, Elizabeth Ligon Bjork, are professors at UCLA; this theory is largely a product of their research.

Any memory has two strengths: a storage strength and a retrieval strength.
  - Storage strength is just that, a measure of how well learned something is. 
    - It builds up steadily with studying, and more sharply with use.
    - The multiplication table is a good example. It’s drilled into our heads in grade school, and we use it continually throughout life: its storage strength is enormous. 
    - According to the Bjorks’ theory, storage strength can increase but it never decreases.
  - Retrieval strength is a measure of how easily a nugget of information comes to mind. 
    - It, too, increases with studying, and with use. 
    - Without reinforcement, however, retrieval strength drops off quickly, and its capacity is relatively small (compared to storage). 
    - At any given time, we can pull up only a limited number of items in connection with any given cue or reminder.
    - Compared to storage, retrieval strength is fickle. It can build quickly but also weaken quickly.
  - No memory, then, is ever “lost” in the sense that it’s faded away, that it’s gone. Rather, it is not currently accessible. Its retrieval strength is low, or near zero. 
  - Compared to a system in which out-of-date memories are overwritten/erased, having memories become inaccessible but remain in storage has important advantages. 
    - Because those memories are inaccessible, they don’t interfere with current information and procedures. 
    - But because they remain in memory they can — at least under certain circumstances — be relearned.
  - The Bjorks argue the brain developed this system for a good reason. 
    - In its nomadic hominid youth, the brain was continually refreshing its mental map to adapt to changing weather, terrain, and predators. 
    - Retrieval strength evolved to update information quickly, keeping the most relevant details handy. It lives for the day. 
    - Storage strength, on the other hand, evolved so that old skills could be relearned, and fast. Seasons pass, but they repeat; so do weather and terrain.
  
Due to the passive side of forgetting (the fading of retrieval strength over time), deeper learning is facilitated once a fact or memory is found again.
  - The harder we have to work to retrieve a memory, the greater the subsequent spike in retrieval and storage strength (learning). 
  - The Bjorks call this principle desirable difficulty.

Using memory changes memory—and for the better. Forgetting enables and deepens learning: 
  - by filtering out distracting information, and 
  - by allowing some breakdown that, after reuse, drives retrieval and storage strength higher than they were originally.

## Breaking Good Habits: The Effect of Context on Learning

Reinstatement theory: background music weaves itself subconsciously into the fabric of stored memory. Cue up the same music, and more of those words are likely to resurface.

Having something going on in the study environment, like music, is better than nothing (so much for sanctity of the quiet study room).

The experience of studying has more dimensions than we notice, some of which can have an impact on retention. 
  - The contextual cues scientists describe—music, light, background colors—are annoyingly ephemeral. 
  - They’re subconscious, usually untraceable. 
  - Nonetheless, it is possible to recognize them at work in our own lives.
  - At least when it comes to retention of new facts, the subconscious cues are valuable, too. 
    - Not always: when we’re submerged in analytical work, they’re negligible.
    - Not necessarily all of them: give someone a real hint—like a category name—and it easily trumps the internal cues.

We can easily multiply the number of perceptions connected to a given memory—most simply, by varying where we study.

Daniel Willingham advises students not to work straight from their notes when reviewing for an exam. 
  - Put the notes aside and create an entirely new outline, reorganizing the material
  - It forces you to think about the material again, and in a different way.

## Spacing Out: The Advantage of Breaking Up Study Time

Distributed learning, or "The Spacing Effect" 
  - People learn at least as much, and retain it much longer, when they distribute—or “space”—their study time than when they concentrate it.
  - Distributed learning, in certain situations, can double the amount we remember later on.

For much of the last hundred years psychologists have confined the study of spacing to short lab experiments.
  - Only in the last several years have researchers mapped out the best intervals to use when spacing study time. 
  - Is it more efficient to study a little bit today and a little bit tomorrow, or to do so every other day, or once a week? What if it’s Tuesday, and the history final is on Friday? What if the exam is a month away? Do the spacing intervals change depending on the exam date?

Jost’s Law
  - If two associations are of equal strength, but of different age, a new repetition has a greater value for the older one.
  - That is, studying a new concept right after you learn it doesn’t deepen the memory much, if at all; studying it an hour later, or a day later, does.

Why spaced study sessions have such a large impact on learning is still a matter of debate. Several factors are likely at work, depending on the interval. 
  - With very short intervals—seconds or minutes, as in the early studies—it may be that the brain becomes progressively less interested in a fact when it’s repeated multiple times in rapid succession.
  - For intermediate intervals of days or weeks, other factors might come into play. Recall the Forget to Learn theory, which holds that forgetting aids learning in two ways: 
    - actively, by filtering out competing facts, and 
    - passively, in that some forgetting allows subsequent practice to deepen learning, like an exercised muscle.

MSpaced study also adds contextual cues.
  - e.g. You initially learned new neighbors' names at the party, surrounded by friends and chatter, a glass of wine in hand. The second time, you heard them yelled out, over the hedges.

With longer intervals of a month or more, and especially with three or more sessions, we begin to notice some of the advantages that spacing allows, because they’re obvious.
  - With longer spaces, you’re forgetting more, but your brain finds out what your weaknesses are and you correct for them.
  - It finds out which mediators — which cues, which associations, or hints you used for each word — are working and which aren’t. And if they’re not working, you come up with new ones.

The more time to prepare, the larger the optimal interval between sessions one and two. That optimal first interval declines as a proportion of the time-to-test, the Internet study found. 
  - If the test is in a week, the best interval is a day or two (20 to 40 percent). 
  - If the test is a month away, then the best option is today, a week from today (for two sessions); for a third, wait three more weeks or so, until a day before the test. 
  - If it’s in six months, the best interval is three to five weeks (10 to 20 percent). 
  - Wait any longer between study sessions, and performance goes down fairly quickly.

Remember, spacing is primarily a retention technique.
  - Having more facts on board could very well help with comprehension, too, and several researchers are investigating just that, for math as well as other sciences. 
  - For now, though, this is a memorization strategy.

## The Hidden Value of Ignorance: The Many Dimensions of Testing

The fluency illusion 
  - The belief that, because facts or formulas or arguments are easy to remember right now, they’ll remain that way tomorrow or the next day. 
  - The sentiment that, once we feel we’ve nailed some topic or assignment, we assume that further study won’t help. 
  - We forget that we forget.

Fluency misperceptions are automatic. 
  - They form subconsciously and make us poor judges of what we need to restudy, or practice again. 
  - We know that if you study something twice, in spaced sessions, it’s harder to process the material the second time, and so people think it’s counterproductive
  - But the opposite is true: you learn more, even though it feels harder. 
  - Fluency is playing a trick on judgment.

The Bjorks’ “desirable difficulty” principle: The harder your brain has to work to dig out a memory, the greater the increase in learning (retrieval and storage strength).
  - Fluency, then, is the flipside of that equation. 
  - The easier it is to call a fact to mind, the smaller the increase in learning. 
  - Repeating facts right after you’ve studied them gives you nothing, no added memory benefit. 
  - The fluency illusion is the primary culprit in below-average test performances.

The best way to overcome this illusion and improve our testing skills is, conveniently, an effective study technique in its own right.
  - The technique is testing itself. 
  - A test is not only a measurement tool: it alters what we remember and changes how we subsequently organize that knowledge in our minds. 
  - And it does so in ways that greatly improve later performance.

Arthur Gates was interested in, among other things, how the act of recitation interacts with memory. 
  - For centuries, students who received a classical education spent untold hours learning to recite from memory epic poems, historic monologues, and passages from scripture. 
  - Gates wanted to know whether there was an ideal ratio between reading (memorizing) and reciting (rehearsal).
  - “In general,” he concluded, “the best results are obtained by introducing recitation after devoting about 40 percent of the time to reading. Introducing recitation too early or too late leads to poorer results,” Gates wrote. 
  - In the older grades, the percentage was even smaller, closer to a third. “The superiority of optimal reading and retention over reading alone is about 30 percent.”
  - The quickest way to download that St. Crispin’s Day speech, in other words, is to spend the first third of your time memorizing it, and the remaining two thirds reciting.

One of the biggest questions hanging over teachers, from the very beginning of the profession, was when testing is most effective. 
  - Is it best to give one big exam at the end of a course? 
  - Or do periodic tests given earlier in the term make more sense?

Herbert F. Spitzer, at the State University of Iowa, found:
  - Groups that took pop quizzes soon after reading a passage once — 1-2x within the first week — did the best on a final exam given at the end of two months, getting about 50 percent of the questions correct. 
  - Groups who took their first pop quiz two weeks or more after studying scored much lower, below 30 percent on the final. 
  - Spitzer showed not only that testing is a powerful study technique, he showed it’s one that should be deployed sooner rather than later.

Henry Roediger III and Jeffrey Karpicke at Washington University, in a landmark 2006 review of "the testing effect," found:
  - “testing” prep buried the “study” prep when it really mattered, on the one-week test
  - In short, "testing != studying"; rather, "testing > studying," and by a country mile, on delayed tests.

In a 2006 review, Roediger and Karpicke analyzed a century’s worth of experiments on all types of retention strategies (e.g. spacing, repeated study, and context)
  - They showed that the testing effect has been there all along, a strong, consistent “contaminant,” slowing down forgetting. 
  - To measure any type of learning, after all, you have to administer a test. 
  - Yet if you’re using the test only for measurement, like some physical education push-up contest, you fail to see it as an added workout — itself making contestants’ memory muscles stronger.

The word “testing” is loaded, in ways that have nothing to do with learning science.
  - In part to soften this resistance, researchers have begun to call testing “retrieval practice.” 
  - That phrase is a good one for theoretical reasons, too: if self-examination is more effective than straight studying (once we’re familiar with the material), there must be reasons for it. 
  - Per the Bjorks’ desirable difficulty principle: when the brain is retrieving previously-studied content, it’s doing something different (and harder) than when it sees the information again/"restudies."
  - That extra effort deepens the resulting storage and retrieval strength. We know the facts or skills better because we retrieved them ourselves, we didn’t merely review them.

Pretesting, the latest permutation of the testing effect
  - If, on Day 1 of a course, you took a test that was comprehensive but not a replica of the final exam? 
  - You’d fail, to be sure; you might not understand a single question.
  - Yet that experience, given what we know about testing, might alter what you subsequently pay attention to and process in the course itself, over the rest of the semester.

Guessing wrongly increases a person’s likelihood of nailing that question, or a related one, on a later test.
  - On some kinds of tests, particularly multiple-choice, you learn from answering incorrectly: especially when given the correct answer soon afterward.
  - Unsuccessful retrieval attempts—i.e., wrong answers—aren’t merely random failures. 
  - Rather, the attempts themselves alter how we think about, and store, the information contained in the questions.
  - “Unsuccessful retrieval attempts potentiate learning, increasing successful retrieval attempts on subsequent tests.” 
    - In plain English: The act of guessing engaged your mind in a different and more demanding way than straight memorization did, deepening the imprint of the correct answers. 
    - In even plainer English, the pretest drove home the information in a way that studying-as-usual did not. 
    - Why? No one knows for sure. 
      - One possible explanation is that pretesting is another manifestation of desirable difficulty. You work a little harder by guessing first than by studying directly. 
      - A second possibility is that the wrong guesses eliminate the fluency illusion, the false impression that you knew the capital of Eritrea because you just saw or studied it. 
      - A third is that, in simply memorizing, you saw only the correct answer and weren’t thrown off by the other four alternatives—the way you would be on a test.

We have all taken practice tests at one time or another as a way of building familiarity with a test-type
  - In that scenario, the practice runs are primarily about reducing anxiety and giving us a feel for format and timing. 
  - The research that the Bjorks, Roediger, Kornell, Karpicke and others have done is different: pre- or post-study, it applies to learning the concepts, terms, and vocabulary that form a knowledge base.
  - “At this point, we don’t know what the ideal applications of pretesting are,” Robert Bjork told me. “It’s still a very new area.”

Apparently-simple attempts to communicate what you’ve learned, to yourself or others, are not merely a form of self-testing in the conventional sense
  - They are, in fact, themselves a form of study (and 20 to 30 percent more powerful than if you continued rereading the outline.) 
  - Better yet, those exercises will dispel the fluency illusion. They’ll expose what you don’t know, where you’re confused, what you’ve forgotten

## Quitting Before You’re Ahead: The Accumulating Gifts of Percolation

Three elements to percolation: 
  - An interruption, 
  - The tuned, scavenging mind that follows, and
  - Conscious reflection. 
 
Research from Zeigarnik, Aarts, Dively, et al takes some of the mystery out of the “creative process.” 
  - Percolation is a matter of vigilance, of finding ways to tune the mind so that it collects (a mix of external perceptions and internal thoughts) that are relevant to the project at hand. 
  - We can’t know in advance what those perceptions and thoughts will look like: and we don’t have to.

It suggests that we should start work on large projects as soon as possible.
  - Stop when you get stuck, with the confidence that you are initiating percolation, not quitting. (Phase 1)
  - A period follows of gathering string, of casual data collecting. (Phase 2) 
  - Phase 3 is listening to what you think about all those incoming bits and pieces.

## Being Mixed Up: Interleaving as an Aid to Comprehension

Kerr and Booth's research:
  - A varied practice schedule may facilitate the initial formation of motor schema,” they wrote, the variation working to “enhance movement awareness.” 
  - In other words: Varied practice is more effective than the focused kind, because it forces us to internalize general rules of motor adjustment that apply to any hittable target.

Psychologists who study learning tend to fall into one of two camps: the motor/movement, or the verbal/academic. 
  - The former focuses on how the brain sees, hears, feels, develops reflexes, and acquires more advanced physical abilities, like playing sports or an instrument. 
  - The latter investigates conceptual learning of various kinds: language, abstract ideas, and problem solving. 
  - Each camp has its own vocabulary, its own experimental paradigms, its own set of theories.
  - This distinction is not an arbitrary one.

A major implication of the Molaison studies was that the brain must have at least two biological systems for handling memory. 
  - One, for declarative memories, is dependent on a functioning hippocampus. 
  - The other, for motor memories, is based in different brain organs; no hippocampus required. 
  - The two systems are biologically distinct, so it stood to reason that they’re functionally distinct, too, in how they develop, strengthen, and fade. 

Transfer is what learning is all about, really. 
  - It’s the ability to extract the essence of a skill or a formula or word problem and apply it in another context, to another problem that may not look the same, at least superficially. 
  - If you’ve truly mastered a skill, you “carry it with you,” so to speak. 

Goode and Magill wanted:
  - To compare the relative effectiveness of each type of practice schedule. 
  - To measure how well the participants’ skills transferred to a new condition. 

Goode and Magill measured transfer in a subtle, clever way. 
  - On their final test of skill, they made one small adjustment: the participants served from the left side of the court, even though they’d practiced only on the right.
  - Interfering with concentrated or repetitive practice forces people to make continual adjustments, they reasoned, building a general dexterity that, in turn, sharpens each specific skill.
  - Goode and Magill then took it one step further. 
    - All that adjusting during a mixed-practice session, they wrote, also enhances transfer. 
    - Not only is each skill sharper; it’s performed well regardless of context, whether indoors or out, from the right side of the court or the left.
  - Whenever researchers scrambled practice sessions, in one form or another, people improved more over time than if their practice was focused and uninterrupted

It’s not that repetitive practice is bad.
  - We all need a certain amount of it to become familiar with any new skill or material. 
  - But repetition creates a powerful illusion. Skills improve quickly and then plateau. 
  - By contrast, varied practice produces a slower apparent rate of improvement in each single practice session but a greater accumulation of skill and learning over time. 
  - In the long term, repeated practice on one skill slows us down.

Forget to Learn theory, Robert and Elizabeth Bjork
  - Any technique that causes forgetting provides “desirable difficulty”, in that 
  - It forces the brain to work harder to dig up a memory or skill, and 
  - That added work intensifies subsequent retrieval and storage strength (i.e. learning).

Interleaving is the process mixing related but distinct material during study. 
  - Music teachers have long favored a variation on this technique, switching from scales, to theory, to pieces all in one sitting. 
  - So have coaches and athletic trainers, alternating endurance and strength exercises to ensure recovery periods for certain muscles. 
  - These philosophies are largely rooted in tradition, in a person’s individual experience, or in concerns about overuse. 
  - Kornell and Bjork’s painting study put interleaving on the map as a general principle of learning, one that could sharpen the imprint of virtually any studied material.

The mixing of items, skills, or concepts during practice, over the longer term, seems to help us
  - See the distinctions between them, and also 
  - Achieve a clearer grasp of each one individually.

The science suggests that interleaving is, essentially, about preparing the brain for the unexpected.
  - Mixing problems during study forces us to identify each type of problem and match it to the appropriate kind of solution. 
  - We are not only discriminating between the locks to be cracked; we are connecting each lock with the right key. 
  - “The difficulty of pairing a problem with the appropriate procedure or concept is ubiquitous in mathematics,” Rohrer and Taylor concluded.

## Learning Without Thinking: Harnessing Perceptual Discrimination

In 1969, Eleanor Gibson published _Principles of Perceptual Learning and Development_; it established a new branch of psychology: perceptual learning. 
  - Perceptual learning is not a passive absorption, but an active process, in the sense that exploring and searching for perception itself is active. 
  - We do not just see, we look; we do not just hear, we listen. 
  - Perceptual learning is self-regulated, in the sense that modification occurs without the necessity of external reinforcement. 
  - It is stimulus oriented, with the goal of extracting and reducing the information simulation. 
  - Discovery of distinctive features and structure in the world is fundamental in the achievement of this goal.

Computer PLMs are meant for a certain kind of target: discriminating or classifying things that look the same to the untrained eye but are not.
  - They are focused on classifying images (do the elevated bumps in that rash show shingles, eczema, or psoriasis?) or problem
  - These modules are intended to sharpen snap judgments—perceptual skills—so that you “know” what you’re looking at without having to explain why, at least not right away. 
  - In effect, the PLMs build perceptual intuition.

It’s a supplement to experience, not a substitute. 
  - That’s one reason perceptual learning remains a backwater in psychology and education. 
  - Only in the past decade or so have scientists begun to exploit Gibson’s findings
  - Scientists have a lot more work to do before they figure out how, and for which subjects, PLMs are most effective.
  - Perceptual learning is happening all the time, after all, and automatically—and it’s now clear that it can be exploited to speed up acquisition of specific skills.
  - 