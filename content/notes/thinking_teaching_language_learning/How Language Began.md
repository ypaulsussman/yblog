---
title: How Language Began (Everett, Daniel)
date: "2018-11-15"
template: "post"
draft: true
slug: "/posts/how-language-began-notes/"
category: "What I Read"
tags:
  - "Linguistics"
  - "Neuroscience"
description: "Notes from a book I read. (Tagline: \"Tracing crucial shifts and developments across the ages, Everett breaks down every component of speech. Moving on from biology to execution, Everett explores why elements such as grammar and storytelling are not nearly as critical to language as one might suspect.\")"
---

Palaeoanthropologist Ralph Holloway and his colleagues proposed four major stages of hominin brain evolution.

1. The foundational stage, beginning with the split between chimpanzees and hominins... roughly 6–8 million years ago. Three characteristics that distinguish them from their descendants. 
    * First, the lunate sulcus (crescent or moon-shaped groove in the brain) of these creatures is found further towards the front (anterior) portion of the brain. This groove divides the visual cortex from the frontal cortex. Since it is known that the frontal cortex of the brain is required for thinking... the position of the lunate sulcus is indicative of the relative thinking sophistication.
    * [The] part of the brain dedicated to connecting multiple cerebral components [is less-developed]. This portion is called the ‘posterior association cortex’. This area of the brain links multiple regions simultaneously, enabling faster thinking.
    * Finally, the brains of the first hominins were small, 350–450cm3 on average. This likely means that their smaller and more simply organised brains would have been incapable of anything like modern human thought.
2. [The next stage occurred] 3.5 million years ago, with the appearance of _Australopithecus africanus_ and _afarensis._ 
    * The lunate sulcus in these creatures has moved a bit further back, [and] the posterior 
   association cortex is also larger in australopithecines.
    * More specialised areas are becoming evident, along with an expansion in size to around 500cm3
    * [There are also] signs of asymmetry, with the left and right hemispheres taking on different
    specialisations.
3. [The] next evolutionary jump occurs about 1.9 million years ago with the appearance of _Homo erectus_. ...[their] brains began to exhibit a prominent region around Broca’s area, important for sequential actions.
4. Holloway’s final stage... occurred about 500,000 years ago, [when] the brain had reached its maximal size and refinement in specialisation for each hemisphere.

Therefore [although] _Homo erectus_ arrived on the scene with brain asymmetries typical of modern humans... it is important to avoid giving the wrong impression. [It] is important to discuss a few of the ways erectus, for all their relative brilliance, were inferior to sapiens.
    
* They lacked a modern hyoid (Greek for ‘U-shaped’) bone, the small bone in the pharynx that anchors the larynx. [They] had not yet taken on the shape of _sapiens’_ and _neanderthalensis’_ hyoids (these two being virtually identical).
* [Because of their] inability to form the same range of vowels that _sapiens_ can produce, in all likelihood their vowels would have been hard to pick up across distances.
* _Erectus_ faces were more distinguished by prognathism [abnormal protrusion of one or both jaws, especially the lower jaw] than modern humans’,
* The FOXP2 gene, though it is not a gene for language, has important consequences for human cognition and control of the muscles used in speech. (FOXP2... elongates neurons and makes cognition faster and more effective. FOXP2 in modern humans also increases length and synaptic plasticity of the basal ganglia, aiding motor learning and performance of complex tasks.) This gene seems to have evolved in humans since the time of erectus. Possessing a more primitive FOXP2 gene, erectus would have had less laryngeal and therefore less emotional control in their speech. 

[It is therefore] probable that _erectus_ was a dull, non-inventive creature compared to modern humans. That doesn’t mean that it was a languageless creature.

---

There is no convincing evidence to date that there are specifically heritable linguistic deficits. Language deficits are rooted in other physical or mental problems.

**Neuroplasticity**... is in part the ability of neurons to change to better fit the needs of their containing organism. 

And there is, of course, also **synaptic plasticity** – the ability of connections (synapses) between neurons to alter as humans learn, grow, or suffer brain damage.

[The] average human brain burns around 325–350 calories per day. That is about one-fourth of the average human’s daily calorie consumption at rest (1,300). We [also] consume higher levels of dietary fat than other primates, and much higher levels of key long-chain polyunsaturated fatty acids (LC-PUFAs) that are critical to brain development.

The **Encephalisation Quotient** ...is is a ratio of a species’ brain size to the average brain size of a mammal with the same body size. Intelligence grows not so much with the absolute size of the brain (a sperm whale’s is around 8,000cm3), but with the ratio of the species’ brain size to its body size.

Tom Schoenemann from Indiana University has made the case that absolute brain size also matters, because it leads to specialisation in the brain that smaller brains are unable to achieve.

Suzana Herculano-Houzel [has] made the case that human brains are superior due in part to much greater neuronal density – we have more neurons per cubic centimetre overall and more connections between them.

The most important question about our brains is not, ‘What in the brain makes language possible?’ The right question is, ‘How do brains, cultures and their interactions work together to produce language?’

---

There were several changes in the environment that pressured human brains to enlarge in order to support greater intelligence.

**Robin Dunbar**, a British anthropologist, claims that the main force driving hominins to develop 
greater intelligence was increased social complexity.

Dunbar’s argument, then, has to do with the exponential growth in the number of social relationships that arises from even the modest increases in overall group size.

Individual members of a society are like neurons in the brain. The more there are, the more connections between them. 

In other words, just as it is the relationships between neurons that make the brain so complex, so it is the exponential growth in relationships as societal numbers increased arithmetically that required more intellectual horsepower in order to keep track of those relationships.

Dunbar noted that cortex size co-varies with group size across several species. Of course, one could reply that Dunbar has the cart before the horse here.

The causality seems more likely to go Dunbar’s way: the brain growing first could have led to any number of social models. But if society grew first then it would have indeed pressured the brain to be able to keep track of the new relationship sizes.

The strongest force for the evolution of human intelligence was in all likelihood a combination of language and culture, as it is manifested through the use of symbols, grammar, pitch and gestures.

---

Neither the brain nor the vocal apparatus evolved exclusively for language. They have, however, undergone a great deal of microevolution to better support human language.

**Basal ganglia** are a group of brain tissues that appear to function as a unit and are associated with a variety of general functions such as voluntary motor control, procedural learning (routines or habits), eye movements and emotional function. This area is strongly connected to the cortex and thalamus, along with other brain areas. These areas are implicated in speech and throughout language. Philip Lieberman refers to the disparate parts of the brain that produce language as the **Functional Language System.**

The basal ganglia (sometimes referred to as the ‘reptilian brain’)... are crucial for language because harm to them produces a number of aphasic conditions.

Language is at least partially a series of acquired habits and routines, along with others like skiing, bicycle-riding, typing and so on, since habits and routines are the purview of the basal ganglia.

The brain is built for learning. It is always best to consider learning as the reason for any information found in any part of the brain, at least before claiming it is hardwired knowledge.

There are indeed some parts of the brain that are linked to language. There must be, in fact. But they are not generally dedicated exclusively to language. 

The lungs, larynx, teeth, tongue, nose and so on are all vital for non-signed language, just as the hands are crucial for signed languages, but none of these are either individually or collectively language organs.

It is often claimed that there are language-specific areas of the brain such as Wernicke’s area or Broca’s area. There are not.

**Broca’s area** is not only not exclusively dedicated to language but also regularly engaged during a range of cognitive tasks, such as coordination of motor-related activities.

Localizing Broca’s region in the context of [a functional imaging study analyzing linguistic material, or a lesion study of a Broca aphasic] may refer to completely different areas with different cytoarchitecture, connectivity and, ultimately, function.

**Wernicke’s area** is connected to other areas of the brain that are, as was the case with Broca’s, far more general in their function than language, such as motor control, pre-motor organisation of potential activities – things like getting your fingers ready to play the guitar before you begin to play.

Wernicke’s area, just as Broca’s area, is [also] part fiction, as there are no agreed-upon definitions of either the location or its extent. 

Research hints that brains are composed of polyvalent (doing more than one thing) networks, along the lines of the Functional Language System, that can reform or be reused for a variety of diverse functions. Recent findings in research at MIT argue that the ‘visual cortex’, the region of the brain usually associated with vision in sighted individuals, can be used for non-visual tasks.

Such work is... important for anyone tempted to conflate statements like ‘this region of the brain does X, among other things’ vs ‘this region of the brain is genetically specified to do X and X alone’.

---

A **transcription factor** is simply a protein that connects (‘binds’) to specific sequences of DNA. By doing this, these factors are able to determine the rate of transcription. That is how information from the genes is passed from DNA to messenger RNA. They regulate how genes are manifested or ‘expressed’.

The brain’s three pounds consist of neurons, glial cells and blood vessels.

**White matter** is named for the white (because fatty) material (technically, _myelin sheaths_) that surrounds the nerve fibres connecting parts of the brain used for higher cognitive functions.

**Glial cells** and mast cells form part of the brain’s _neuroimmune system,_ independent of the immune system that protects the rest of the body.

There are some 100 billion neurons in the average brain. There are also non-neuronal cells of a roughly equal amount. 

Nearly 69 billion of our brain neurons, 80 per cent of all our neurons, are located in the cerebellum or ‘little brain’, which sits just below the [cerebrum].

The biggest part of the brain is the cerebrum, broken into two hemispheres. Each of the cerebral hemispheres is divided into four lobes; their primary purpose is to refer to a general region. They are too poorly understood and demarcated to attribute to them any significant function

No two brains have exactly the same pattern of gyri and sulci. Yet human brains all function similarly, in spite of gyri and sulci patterns.

**Cytoarchitecture** ...is the cover term for the differences in the construction of individual cells found in particular brain regions.

[A **Brodmann area** is a region of the cerebral cortex, defined by its cytoarchitecture.] This organisation is manifested in various distinct ways – connections between cells, shapes of cells or cell parts, thickness of the cortex in a particular cellular region, according to which region of the cortex we are viewing and what function that region has.

It is possible that some areas of the cytoarchitecture of the brain are indeed propitious for storing or operating syntax. But that is not the same as saying that BA 44 and its temporal lobe connection are the evolutionary developments that pulled language out of a primate brain. BA 44 has at least six separate functions, including sound or phonological processing, syntactic processing, understanding meaning or semantic processing, and music perception.

---

There are three varieties of **sensory memory:** 
1. _echoic,_ for sounds; 
2. _iconic,_ for vision; and 
3. _haptic,_ for tactile sensations. 

Another kind of memory, short-term or **working memory,** is also crucial in the use of language.

George A. Miller's... research concluded that, without practice, people can remember up to nine, usually more like five, items at a time for roughly a minute. Miller discovered, however, that if items are ‘chunked’, then people are able to remember larger numbers of items.

Working memory is biased towards sound-based memories, which means not only that it is important for remembering and decoding utterances, but also that it seems to have partially evolved for that very purpose, language once again helping to shape human evolution.

**Long-term memory** is divided into declarative memory and procedural memory.
1. **Procedural memory** is implicit memory of processes involving motor skills. When trying to remember a password, your declarative memory might fail you, in the sense that you cannot recall the names of all the symbols you have chosen for your password. But your procedural memory can come to the rescue if you will simply set yourself down at the keyboard and type the password. Procedural memory is vital for pronunciation or for gestures in sign languages, providing much quicker use and access to words and signs than declarative memory, just as one’s fingers might better remember their computer password.
2. **Declarative memory** is subdivided into semantic memory and episodic memory. _Semantic memory_ is associated with facts independent of any context, such as ‘a bachelor is an unmarried male’. It is crucial for linguistic meanings. _Episodic memories,_ on the other hand, are long-term memories associated with a specific context and, therefore, tend to be more personal. You might use this memory to recall, ‘That’s where we had our first water pistol fight.’

Working memory takes place as exchanges between neurons in the frontal cortex. But long-term memories are widely distributed in the brain and seem to be processed initially by the hippocampus, which ‘consolidates’ memory for long-term storage elsewhere.

The brain is ‘embodied’ – built into an anatomical, chemical, electrical and physically constrained system... it is not so much that the brain thinks as that the entire individual does.

---

Each of the three steps in the creation and transmission and understanding of speech has an entire subfield of **phonetics**, the science of sounds, dedicated to it. 
1. The creation of sounds is the domain of the field of ‘articulatory phonetics’. 
2. The transmission of sounds through the air is ‘acoustic phonetics’. 
3. And the hearing and interpretation of sounds is ‘auditory phonetics’.

[_Side note:_] Another way of talking about ‘lax’ vs ‘tense’ vowels, one preferred by many linguists, is to refer to them as ‘Advanced Tongue Root’ (the tongue is thereby tensed by being pushed forward in the mouth and flexing) or ‘Not Advanced Tongue Root’ (the tongue is relaxed, its root further back in the mouth), usually written in the linguistics literature as [+ATR] or [−ATR].

The energising of the flow of air in speech by the larynx is known as **phonation,** which produces for each sound what is known as the ‘fundamental frequency’ of the sound. 

The **fundamental frequency** is the rate of vibration of the vocal folds during phonation and this varies according to the size, shape and bulk (fat) of the larynx. Small people will generally have higher voices, that is, higher fundamental frequencies, The fundamental frequency, usually written as F0, is one of the ways that people can recognise who is talking to them.

In addition to the fundamental frequency, as each speech sound is produced harmonic frequencies, or **formants,** are produced that are associated uniquely what that particular sound. These formants enable us to distinguish the different consonants and vowels of our native language. One does not directly hear the syllable [dad], for example. What is heard are the formants and their changes associated with these sounds.

According to evolutionary research, the larynges of all land animals evolved from the same source – the lung valves of ancient fish, in particular as seen in the _Protopterus,_ the _Neoceratodus_ and the _Lepidosiren._ Fish gave speech as we know it. The two slits in this archaic fish valve functioned to prevent water entering into the lungs of the fish.

---

Linguists call the placing together of meaningless sounds (‘phonemes’ is the name given to speech sounds) into meaningful words **‘duality of patterning’.*** 
* For example the `c`, `a` and `t` of the word ‘cat’ are meaningless on their own.
* Thus a [meaningful] word such as ‘cat’, a symbol, is organised horizontally, or syntagmatically, as a syllable, c-a-t. 
* But... ‘cat’ is organised vertically at the same time. So one could substitute a ‘p’ for the ‘c’ of cat to produce the word 'pat'. Or one could substitute ‘d’ for ‘t’ and get instead 'cad'. 

The **syllable** is therefore itself an important part of the development of duality of patterning. 

It is a natural organising constraint set on the arrangement of phonemes that works to enable each phoneme to be better perceived.

The order preferred most of the time is that, from left to right in the syllable, the sounds are arranged from least inherently loud to the loudest and then back to softest. This makes the sounds in each syllable easier to hear. It is another way of chunking that helps our brains to keep track of what is going on in language. 

This property is called **sonority.** In simple terms, a sound is more sonorous if it is louder. Consonants are less sonorous than vowels. And among consonants some... are less sonorous than others.

[_Side note:_] English [has] very complicated syllable patterns. The word ‘strength’ has more than one consonant in each margin. And the consonant ‘s’ should follow the consonant ‘t’ at the beginning of ‘strength’ because it is more sonorant. So the word should actually be ‘tsrength’. It does not take this form because English has historically preferred the order ‘st’, based on sound patterns of earlier stages of English and the languages that influenced it, as well as cultural choice.

---

The human vocal apparatus has three basic components:
1. moving parts (the articulators), 
2. stationary parts (the points of articulation), and 
3. airflow generators.

Individual speech sounds, phones, are produced by the articulators – tongue and lips for the most part – meeting or approximating points of articulation – alveolar ridge, teeth, palate, lips and so on. 
* Some of these sounds are louder because they offer minimal impedance to the flow of air out of the mouth (and for many out of the nose). These are vowels.
* Other phones completely or partially impede the flow of air out of the mouth. These are consonants.

[_Side note:_] The inability of neanderthalensis to produce /i/, /a/ and /u/ (at least according to Philip Lieberman) would be a handicap for speech, but these ‘cardinal’ or ‘quantal’ vowels are neither necessary nor sufficient for language (not necessary because of signed languages, not sufficient because parrots can produce them).

Since speech is... a continuous stream of articulatory movements, ‘assimilate’ to one another, that is they become more alike in some contexts, though not always the same contexts in every language. If a native speaker of English utters the word ‘clock’, the ‘k’ at the end is pronounced further back in the mouth than it is when they pronounce the word ‘click’. This is because the vowel ‘o’ is further back in the mouth and the vowel ‘i’ is further to the front of the mouth. In these cases, the vowel ‘pulls’ the consonant towards its place of articulation.

The study of the emic knowledge of speakers, what enhancements are ignored by native speakers and what sounds they target, is phonology.

[_Side note:_] Phonology is, like all other forms of human behaviour, constrained by the **memory–expression tension:** the more units that a language has, the less ambiguously it can express messages, but the more there is to learn and memorise.

[_Side note:_] The stream of sounds produced by any speaker can be organised so as to maximise both information rate (consonants generally carry more information than vowels, since there are more of them) and perceptual clarity (consonants are easier to perceive in different positions of the speech stream, such as immediately preceding and following vowels and the beginnings and ends of words).

---

It is the ability to encode or decode a meaning of a whole utterance from the individual meanings of its parts.

Kenneth Pike placed morphology and syntax together in... the ‘morphosyntactic hierarchy’ – the building up of conversations from smaller and smaller parts.

Syntax develops with... cultural communicational objectives and conventions, along with different grammatical strategies. This means that one can add recursion if it is a culturally beneficial strategy; one can have relative clauses (or conjoined noun clauses), or not:
* John and Mary went to town (a complex, conjoined noun phrase) vs John went to town. Mary went to town (two simple sentences). 
* The man is tall. The man is here (two simple sentences). vs The man who is tall is here (a complex sentence with a relative clause).

Morphology is the scientific term for how words are constructed.

Thus in English there are at most five different forms for any verb: _sing, sang, sung, singing, sings._

Words can be simple or composed of parts (called morphemes). 
* If they are simple, without internal divisions, this is an ‘isolating’ language. Chinese is one illustration.
* Romance (or Slavic) languages... are referred to linguistically as ‘inflectional’ languages.
* Turkish and many Native American languages are called ‘agglutinative’. This means that each part of each word often has a single meaning, unlike Romance languages, where each part of the word can have several meanings.
* Or one could express some meaning on the consonants and another part of the meaning of the word on the vowels, in which case we have a 'non-concatenative' language (Arabic languages are of this type... but English also has examples: foot is singular, but feet is plural.)

Languages tend to mix different approaches to building words.

---

Why don’t people mix gestures or other noises with speech sounds in their grammars? Why is it that only sounds made by the mouth can be used in syllables and speech more generally? Why couldn’t a word like ‘slap’ be [sla#], where [#] would represent the sound of someone slapping their chest?

[_Side note:_] Italians use gestures to signal and support content. For example, a ‘deep’ valley, a ‘tall’ man, ‘no way’. The Jewish immigrants of Efron’s study, on the other hand, use gestures as logical connectives, that is to indicate changes of scene, logical divisions of a story and so on.

David McNeill and Adam Kendon classify all these different forms along a ‘gesture continuum’. [This continuum covers the degree to which movements are fleeting creations of the moment, inextricably tied to concurrent speech, as opposed to autonomous, "language-like" signaling devices, independent of verbalization.]

[A] catchment is recognized when one or more gesture features occur in at least two (not necessarily consecutive) gesture events. The logic is that recurrent images suggest a common discourse theme: a catchment is a kind of thread of visuospatial imagery that runs through a discourse to reveal the larger units that encompass the otherwise separate parts.

The connection (and synchrony) between gestures and speech is also culturally malleable. Field researchers have demonstrated that the Arrernte people of Australia regularly perform gestures after the speech... [for] the Turkana people of Kenya... gestures function to echo and reinforce speech.

---

The British philosopher Paul Grice developed some helpful concepts for understanding the cultural and communicational presuppositions that underlie all human communication, which he referred to in the aggregate as the ‘cooperative principle’. 

As Grice said, summing up his ideas, ‘Make your contribution such as it is required, at the stage at which it occurs, by the accepted purpose or direction of the talk exchange in which you are engaged.’ 

Grice makes this look like advice or a command, but it is actually intended as just a description of the cultural conventions underlying communication. We don’t have to be taught these things. This is how we behave.

1. The maxim of quality assumes that everyone will speak the truth. 
  * It presumes that neither the hearer nor the speaker will believe that anything will be presented as true if it is known to be false. 
  * It also assumes that no one will say that something is true if they lack adequate evidence.

2. The next of the four maxims Grice lays out is the maxim of quantity.
  * First, don’t give any more information than the exchange requires. 
  * Second, relay all the information necessary for the current interaction.

3. [Maxim of relation: be relevant.]

4. Grice’s maxim of manner: the interlocutors assume that each intends to be clear in their speech. People believe that a speaker is making an effort to avoid ambiguity, to be as brief as possible while respecting the maxim of quality and to be orderly in their remarks.

Implicatures, how people interpret the flouting of maxims, are cognitively complex. They draw on a store of background cultural knowledge. For this reason interpreting conversation in light of the cooperative principle is highly culture specific. The maxims themselves, on the other hand, are probably found in all languages.

Oxford don John Austin talked about locutionary acts (what was said), illocutionary acts (what was meant) and perlocutionary acts (what happened as a result of what was said and what was meant).
1. The locutionary act is speaking itself. If one asks, ‘Where is Bill?’ the very moving of the mouth, emission of air from the lungs and the arrangement and selection of the words used are all part of the locutionary act. But anyone performing this locutionary act is simultaneously performing an illocutionary act. An illocutionary act is the effect one intends their utterance to have.
2. The illocutionary acts a person’s words can accomplish include statements, commands, questions, or performative acts. The latter occurs when a minister, legally authorised to perform marriages, concludes a legitimate (non-faked, non-Hollywood) marriage ceremony
3. Finally, there is the perlocutionary act – what happens, or what one hopes will happen, in the mind of my hearer when they speak.

If one speaks or translates or otherwise engages in the communicative enterprise they are hoping that their locutionary act will be the right choice for the right illocutionary act to produce the desired perlocutionary effect or act.

In 1850 twenty-nine-year-old German philologist August Schleicher published a book in which he claimed that human languages should be studied as organisms... that the best way to represent the evolutionary relationships between languages was by ‘tree diagrams’. With this proposal he not only made an enormous contribution to the history and evolution of languages but also introduced the concept of ‘natural descent’ – nine years before Darwin published his Origin of Species.

Glottochronology, invented by linguist Morris Swadesh, assumed that there were some vocabulary items (such as parts of the body, words for sun, moon and others) that were less likely to be borrowed. He therefore came up with a list of two hundred words or ‘lexical items’ which he considered represented the words least likely to change. A mathematical formula was proposed and developed, based on the rate of change of the words in his list, to predict the rate at which these most resistant-to-change words might in fact change over time. The formula was tested and deemed to have 87 per cent accuracy in known cases, such as the Indo-European languages.

