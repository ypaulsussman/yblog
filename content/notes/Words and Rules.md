---
title: Words and Rules (Pinker, Steven)
date: "2019-01-18"
template: "post"
draft: true
slug: "/posts/words-and-rules-notes/"
category: "What I Read"
tags:
  - "Linguistics"
  - "Neuroscience"
description: "Notes from a book I read one winter. (Tagline: \"In 'Words and Rules', Steven Pinker explains the mysteries of language by examining a single construction from a dozen viewpoints, proposing that the essence of language is a mental dictionary of memorized words, and a mental grammar of creative rules.\")"
---

[_We'll be_] choosing a single phenomenon and examining it from every angle imaginable. That phenomenon is regular and irregular verbs...

Seeing the world in a grain of sand is often the way of science, as when geneticists agreed to study the lowly fruit fly so that their findings might cumulate into a deep understanding that would have been impossible had each scientist started from scratch with a different organism. Like fruit flies, regular and irregular verbs are small and easy to breed, and they contain, in an easily visible form, the machinery that powers larger phenomena in all their glorious complexity.

---

### Introduction: Words and Rules

The premise of this book is that there are two tricks, words and rules.

Their border disputes shape and reshape languages over centuries, and make language not only a tool for communication but also a medium for wordplay and poetry and an heirloom of endless fascination.

* The first trick, **the word**, is based on a memorized arbitrary pairing between a sound and a meaning.
  * What’s in a name is that everyone in a language community tacitly agrees to use a particular sound to convey a particular idea. 
    * Although the word _rose_ does not smell sweet or have thorns, we can use it to convey the idea of a rose because 
    * all of us have learned, at our mother’s knee or in the playground, the same link between a noise and a thought. 
    * Now any of us can convey the thought by making the noise.
  * Ferdinand de Saussure, a founder of modern linguistics, called such pairing the arbitrary sign and made it a cornerstone of the study of language.
    * Children begin to learn words before their first birthday, and 
    * by their second they hoover them up at a rate of one every two hours. 
    * By the time they enter school children command 13,000 words, and then the pace picks up, because new words rain down on them from both speech and print. 
    * A typical high-school graduate knows about 60,000 words; a literate adult, perhaps twice that number.
  * People recognize words swiftly. 
    * The meaning of a spoken word is accessed by a listener’s brain in about a fifth of a second, before the speaker has finished pronouncing it.
    * The meaning of a printed word is registered even more quickly, in about an eighth of a second.
  * People produce words almost as rapidly: 
    * it takes the brain about a quarter of a second to find a word to name an object, and 
    * about another quarter of a second to program the mouth and tongue to pronounce it.

* Modern linguists call the [_set of rules that specifies how words may be arranged into meaningful combinations_] a... **generative grammar** to distinguish it from the grammars used to teach foreign languages or to teach the dos and don’ts of formal prose.
  * First, the rules are productive. 
    * By specifying a string of kinds of words rather than a string of actual words, the rules allow us to assemble new sentences on the fly and not regurgitate preassembled clichés—and 
    * that allows us to convey unprecedented combinations of ideas. Though we often speak of roses being red, we could talk about violets being red if the desire came over us (perhaps to announce a new hybrid),
  * Second, the symbols contained by the rules are symbolic and hence abstract. 
    * The rule doesn’t say, “A sentence may begin with a bunch of words referring to a kind of flower”; rather, 
    * it says, “A sentence may begin with an `NP`,” where `NP` is a symbol or variable that can be replaced by any noun, 
    * just as `x` or `y` in a mathematical formula can be replaced by any number.
  * Third, the rules are combinatorial. 
    * They don’t just have a single slot, like a fill-in-the-blank exam question; 
    * every position in the sentence offers a choice of words from a lengthy menu. 
    * Say everyday English has four determiners (_a, any, one,_ and _the_) and ten thousand nouns. 
    * Then the rule for a noun phrase allows four choices for the determiner, followed by ten thousand choices for the head noun, yielding 4 x 10,000 = 40,000 ways to utter a noun phrase.
    * Combinatorial systems obey what Miller calls the Exponential Principle: The number of possible combinations grows exponentially (geometrically) with the size of the combination
  * Suppose our rule for the verb phrase is enriched to allow a sentence to appear inside it, as in _I told Mary he was a fool_, in which _he was a fool_ comes after the `object NP` _Mary_. Now our grammar is recursive: the rules create an entity that can contain an example of itself.

---

### Digression: Why Rules Don't Take Over

The idea that the creativity inherent in language can be explained by a grammar of combinatorial rules is usually associated with the linguist Noam Chomsky. 

Chomsky traced the idea to Wilhelm von Humboldt, a nineteenth-century pioneer of linguistics, who explained language as “the infinite use of finite media.”

Why has no modern language used the horsepower of combinatorial grammar to the fullest and abandoned the unprincipled, parochial, onerous-to-memorize laundry list called vocabulary? 

The answer becomes clear when we look at the most famous of the combinatorial schemes of the Enlightenment, the philosophical language of Bishop John Wilkins.
* Wilkins’s system, laid out in a lengthy 1668 opus, offered the user a non-arbitrary name for every thing by dividing the universe into categories and subcategories and sub-subcategories, and assigning a vowel or consonant to every branch in the tree. 
  * The first syllable identified one of the forty categories into which Wilkins had sorted all thinkable thoughts.
  * For example, Z stood for “sensitive species” (animals) and could be followed by i for “beasts” (quadrupeds). The next consonant picked out a subdivision; t, for example, stood for rapacious terrestrial European canines. A final vowel pinpointed the species, yielding Zita as the name for dogs.
* [_This system_] forces users to perform a chain of computations in their heads every time they want to refer to a dog. Every vowel and consonant is laden with meaning and acts as a premise in a lengthening deduction.
* A second problem is that there are more things in heaven and earth than were dreamt of in Wilkins’s philosophy, which identified only two thousand concepts. Wilkins’s dilemma was that 
  * he could either expand his system to embrace all concepts, which would require even longer and more unwieldy strings, or 
  * he could force his users to remember the nearest synonym, reintroducing the despised memorization process.
* A third problem is that in a logical language words are assembled purely on information-theoretic principles, with no regard to the problems that incarnate creatures might have in pronouncing and understanding the strings.
* Another problem is that Wilkins’s words are packed tight with information and lack the safety factor provided by redundancy. The slightest slip of the tongue or pen guarantees misunderstanding.
* Finally, all that power is not being put to any sensible use. 
  * The beauty of a combinatorial system is that it generates combinations that have never before been considered but that one might want to talk about some day. 
  * [_Moreover,_] familiar objects and actions around us often form a noncombinatorial list of distinctive kinds. When we merely have to single out one of them, a combinatorial system is overkill. We never will have to refer to fish with an enmity to sheep...

No language works like Wilkins’s contraption, with every word compiled out of meaningful vowels and consonants according to a master formula. All languages force their speakers to memorize thousands of arbitrary words, and now we can see why.

---

### Verbs: Our Window to Words and Rules

[But] perhaps words and rules are two modes of operation of a single faculty. Or perhaps there is a gradual continuum between memory and combination... We need side-by-side specimens in which the same kind of thought is packed into the same kind of verbiage, but one specimen shows the handiwork of a word regurgitator and the other shows the handiwork sof a rule amalgamator. I believe that languages do provide us with such specimens. They are called regular and irregular words.

[_In_] contrast to the regulars, irregular verbs form a closed list. There are only about 150 to 180 irregular verbs in modern English (depending on how you count), and there have been no recent additions.

Regular past-tense forms are predictable in sound and generated freely because they are products of a rule that lives in the minds of children and adults: “The past tense of a verb may be formed from the verb followed by the suffix _-ed_.”

Irregular verbs, in contrast, are unpredictable in form and restricted to a list because they are memorized and retrieved as individual words. An irregular form would look just like the lexical entry we saw when considering the name of the rose.

[_There is a_] simple principle: If a word can provide its own past tense from memory, the rule is blocked; elsewhere (by default), the rule applies.

The advantage of a rule is that a vast number of forms are generated by a compact mechanism. In English the savings are significant: The rules for _-ed, -s,_ and _-ing_ (the three regular forms of the verb) cut our mental storage needs to a quarter of what they would be if each form had to be stored separately.

On the other hand, a rule is more powerful than needed for words we hear so often that retrieval from memory is easy. As we shall see, it is the most common verbs, such as _be, have, do, go,_ and _say,_ that turn out to be irregular in language after language. 

Rules have another shortcoming... A sequence of sounds that encodes a concept precisely and efficiently may be unresolvable by the ear or unpronounceable by the tongue. 
  * So it is with the rule for the past tense in English. The delicate tongue-tap that graces the end of a regular form may escape a listener and be omitted when he reproduces [_it_]. Irregular verbs, in contrast, tend to use vowel changes such as ring—rang, strike—struck, and blow—blew, which are as clear as a bell.
  * [_The rule also_] can let it blindly jam a suffix onto the end of an inhospitable sound. The result can be an uneuphonious tongue-twister such as edited or sixths.

---

### Lexicon and Morphology

* [Three] components pass messages about meaning back and forth with the rest of the mind so that the words correspond to what the speaker wants to say. This interface between language and mind is called **semantics**. 
  * One is a storehouse of memorized words, the mental **lexicon**. 
  * Another is a team of rules that combine words and parts of words into bigger words, a component called **morphology**. 
  * A third is a team of rules that combine words into phrases and sentences, a component called **syntax**. 
* Finally, the assembled words, phrases, and sentences are massaged by a set of rules into a sound pattern that we can pronounce when speaking or extract from the stream of noise when listening. This interface between language and the mouth and ear is called **phonology**.

The word _word_ has two very different senses.
* The first sense matches the everyday notion of a word: a stretch of sound 
  * that expresses a concept, 
  * that is printed as a string of letters between white spaces, and 
  * that may be combined with other words to form phrases and sentences. 
  * Some of these words are stored whole in the lexicon, like _duck_ and _swam_; others are assembled out of smaller bits by rules of morphology such as _quacked_ and _duck-billed platypus_. 
  * A technical term for a word in this sense is a **morphological object**, to be distinguished from phrases and sentences, which are **syntactic objects**. 
* The second sense of word is a stretch of sound that has to be memorized because it cannot be generated by rules. 
  * A chunk of any size that has to be memorized - prefix, suffix, whole word, idiom, collocation — is the second sense of word. 
  * Some memorized chunks are smaller than a word in the first sense, such as prefixes like _un-_ and _re-_ and suffixes like _-able_ and _-ed_. 
  * Others are larger than a word in the first sense, such as idioms, clichés, and collocations. 
    * Idioms are phrases whose meanings cannot be computed out of their parts, such as _eat your heart out_ and _beat around the bush_. 
    * Collocations and clichés are strings of words that are remembered as wholes and often used together, such as _gone with the wind_ or _like two peas in a pod_.
  * It is the sense of word that contrasts with rule.
  * A memorized chunk is sometimes called a **listeme**, that is, an item that has to be memorized as part of a list.
* So _walked_ is a word in the first sense (a morphological object) and not a word in the second sense (a listeme); its listemes are _walk_ and _-ed_. 
  * These one-part listemes — prefixes, suffixes, and the stems they attach to, such as walk — are called **morphemes**, 
  * a term coined by the nineteenth-century linguist Baudouin de Courtenay to refer to “that part of a word which is endowed with psychological autonomy and is for the very same reasons not further divisible.”

What about the rules? Why divide the rules of morphology, which build complex words (including regular plurals and past-tense forms), from the rules of syntax, which build phrases and sentences? Both are productive, recursive, combinatorial systems, and some linguists see them as two parts of a larger system.

How people pluralize an expression depends on how they tacitly analyze it: as a word or as a phrase.
* [In a compound word] The plural still goes on the end: two _cowgirls_, not two _cowsgirl_ or two _cowsgirls_. That is because the word _girl_ inside _cowgirl_ is special. 
  * It is called the head of the word, and it stands for the word as a whole in determining its meaning (a cowgirl is a kind of girl) and in determining its plural: 
  * The _-s_ goes on girl. 
* A phrase also has a head, and it too determines the meaning and gets the plural. But now we discover the major difference between a word, the product of morphology, and a phrase, the product of syntax: 
  * In the phrase, the head is on the left, not the right. 
  * If you meet more than one _girl from Ipanema_ (head = _girl_), they are _girls from Ipanema_, not _girl from Ipanemas_. 
* With a word the plural is on the end (cowgirls); with a phrase the plural can be in the middle (girls from Ipanema).
* The seeds of the mother-in-law dispute were sown by a special option of English: Occasionally a phrase gets repackaged into a long word.
  * When the phrase is used as a word repeatedly, the original meaning can recede from collective memory. The phrase boundaries melt into a glob, and speakers no longer sense its parts.
  * Most of our disputed plurals originated as phrases and then became words. Long ago people might have thought, “she is not my mother in reality, she is only my mother in law” (that is, according to canon or Church law). But the concept of a spouse’s mother needs a word, and eventually the phrase got reanalyzed as that word: “She is my mother-in-law.” 
  * If some speakers still hear the phrase inside the word, they will be tempted to put the plural marker on the head of the phrase: two mother + s in law,

Morphology may be divided into:
* _derivation_ — rules that form a new word out of old words, like duckfeathers and unkissable, and 
* _inflection_ — rules that modify a word to fit its role in a sentence, what language teachers call conjugation and declension. 
  * The past tense and plural forms are examples of inflection. 

---

### Inflection, Syncretism, and Allomorphy

English inflection is famous among linguists for being so boring. 
  * In the Bantu language Kivunjo, for example, a verb is encrusted with prefixes and suffixes that multiply out to half a million combinations per verb. 
  * But English speakers subsist on only four: _open, opens, opened,_ and _opening._
  * Strangely enough, English grammar does not have only four roles for verbs to play. It has at least thirteen different roles,
    * The suffix, _-Ø_ is used in four variations of the verb in English: 
      * Present tense, all but third-person singular: I, you, we, they open it. 
      * Infinitive: They may open it, They tried to open it. 
      * Imperative: Open! 
      * Subjunctive: They insisted that it open. 
    * The suffix _-s_ is used for only one purpose: Present tense, third-person singular: He, she, it opens the door. 
    * The suffix _-ing_ is used in at least four ways: 
      * Progressive participle: He is opening it. 
      * Present participle: He tried opening the door. 
      * Verbal noun (gerund): His incessant opening of the boxes. 
      * Verbal adjective: A quietly-opening door. 
    * Finally we come to our friend _-ed_, which has four jobs: 
      * Past tense: It opened. 
      * Perfect Participle: It has opened. 
      * Passive Participle: It was being opened. 
      * Verbal adjective: A recently-opened box.12

One verb comes in eight different forms: 
1. Infinitive; 
1. subjunctive; 
1. imperative: To be or not to be; Let it be; Be prepared. 
1. Present tense, first-person singular: I am the walrus. 
1. Present tense, second-person singular, all persons plural: You/we/they are family. 
1. Present tense, third-person singular: He/she/it is the rock. 
1. Past tense, first- and third-person singular: I/he/she/it was born by the river. 
1. Past tense, second-person singular, all persons plural; 
1. subjunctive: The way we/you/they were; If I were a rich man. 
1. Progressive participle; 
1. present participle;
1. gerund: You’re being silly; It’s not easy being green; Being and Nothingness. 
1. Perfect participle: I’ve been a puppet, a pauper, a pirate, a poet, a pawn and a king.

With nouns, too, different grammatical forms have to dip into the same small pool of suffixes. 
* The naked stem _dog_ must be distinguished from the singular _dog + Ø_ because a dogcatcher doesn’t catch just one dog and a dog lover doesn’t love just one.
* The dog inside these compounds refers to dogs in general and thus differs in meaning from the singular form in a dog.

Regular inflection in English is remarkably simple. 
* All the inflections are suffixes; none of the grammatical roles call for a prefix or some other way of decorating or tinkering with a word. 
* And every word has at most one inflectional suffix. 
  * We never get _opensed_ or _opensing_, 
  * nor do the plural _-s_ and possessive _s_ stack up when several owners own something: _the dogs’ blanket_, not _the dogs’s_ (dogzez) _blanket_. 
* Finally, each niblet of sound making up a suffix has a life of its own and combines with several verb forms, noun forms, or both, rather than being a slave to only one role. This suggests that 
  * instead of crediting English speakers with seventeen verbose rules like “To form the past tense, add -ed to the end of the verb,” 
  * we can credit them with just one rule: “A word may be composed of a stem followed by a suffix,”
* English could have used seventeen different forms for its seventeen slots in the noun declension and verb conjugation: prefixes such as _ib-_, _tra-_, and _ka-_, suffixes such as _-og_, _-ig_, and _-ab_, and so on. 
* Instead the slots share a few sounds (_-0, -ed, -s, -ing_) and one position (immediately following the verb). 
* This miserliness, called **syncretism**, is found in language after language. Syncretism suggests that the mind keeps separate accounts 
  * for the templates that build words (for example, “word = stem + suffix”), 
  * for the scraps of sound that may be added to words (-s, -ed, and -ing), and 
  * for the roles these additions can play (for example, plural, participle, imperative.)
* Syncretism—one form, several roles—is one kind of violation of the simplest conceivable system in which every sound has one meaning and vice-versa. 

The other kind of violation — one role, several forms — is rampant in languages as well; linguists call it **allomorphy**. 
* Though always spelled _-ed_, [the regular past-tense suffix] is pronounced in three different ways. 
  * In _walked_, it is pronounced `/t/`. 
  * In _jogged_, it is pronounced `/d/`. 
  * And in _patted_, it is pronounced `/ɨd/`, where ɨ is a neutral vowel called “schwa.” 
* We also find allomorphy in the regular plural: 
  * The suffix _-s_ has three different forms in _cats, dogs,_ and _horses_. 
* Are there in fact three past-tense suffixes and three plural suffixes?
* One past tense suffix is stored in the lexicon, not three, and a separate module fiddles with its pronunciation: the rules of phonology, which define the sound pattern or accent of a language.

---

### Digression: Suffix Pronunciation

1. English never forces speakers to turn their vocal cords on for one consonant then off for the next, or vice-versa. 
  * We see the restriction in force in one-piece words that end in a cluster of consonants.
  * Not every sound cares about the consonant that follows it. 
    * Those that do are consonants in which the airstream is obstructed, namely _p, h, t, d, k, g, s, sh, ch, z, zh,_ and _th_. 
    * But the vowels, and the vowel-like consonants _r, l, n,_ and _m_, are indifferent to what comes after them; they tolerate either _s_ or _z_, either _t_ or _d_,
2. The vowel appears when _d_ follows _t_ or _d_, and when _z_ follows _s_ or _z_. The word endings that trigger the extra vowel are similar in pronunciation to the suffixes themselves, and that can’t be a coincidence. Apparently a rule is trying to separate too-similar adjacent consonants by pushing a vowel between them: between _t_ and _d_, _d_ and _d_, _s_ and _z_, _z_ and _z_, _sh_ and _z_, and so on.

We even can deduce which of the two rules applies first: 
* The devoicing rule is triggered by adjacent consonants; the vowel rule breaks up adjacent consonants. 
  * If the voicing rule came first, it would convert `/pat/` + `/d/` to `/pat/` + `/t/`, and only then would the vowel be inserted, yielding `/pătɨt/`
  * But that is not how we pronounce it; we say `/pătɨd/`.
* The vowel rule must have come first, creating _patted_; 
  * now the voicing rule is no longer compelled to do anything, because 
  * the `/td/` sequence that would trigger it has been broken up:

Our current system is the result of a reorganization that began around the time of the origin of Modern English in the seventeenth century. 
* Before that, _-ed_ and _-s_ suffixes were pronounced (and spelled) with vowels all the time, not just with words ending in _t_ or _d_ or in _s_ or _z_. 
* For centuries, English speakers had been concentrating stress on the first syllables of words, which shriveled the later syllables, and speakers began to leave out the vowels in the suffixes of many words.
* Writers called attention to the new, clipped pronunciations by spelling them phonetically with an apostrophe in place of the deleted vowel,
* Jonathan Swift deplored the change [but his] contemporary, Samuel Johnson, who was standardizing the spellings of English words in a way that reflected the morphemes that composed them, recognized that _’d_ and _-ed_ were the same morpheme, and obliterated the distinction in their spelling, making _ed_ the spelling for both.
* It is unclear why he chose to leave the _e_ in _-ed_ across the board (_mapped_ and _matted_), but opted to spell _-s_ either with or without an _e_, depending on how it is pronounced (_maps_ and _masses_).

---

### The Origin of Irregular Verbs

The past-tense rule belongs to a component, morphology, that builds things out of parts using rules. 
* The rule itself is a masterpiece of minimalism — “_a word can be composed of a stem and a suffix_” — with 
* all other details distilled out and collected in the lexical entry for the suffix. 
* The suffix itself 
  * is shared among several inflections (past tense, participle, and so on), and 
  * its variant pronunciations (t, d, ɨd) do not wastefully multiply listings but are computed automatically by two ubiquitous rules of phonology. 

The remarkable feat we call **language acquisition**...
1. A generation of speakers uses their lexicon and grammar to produce sentences. 
2. The younger generation listens to the sentences and tries to infer the lexicon and grammar...

That is how irregular forms, in particular, come down to us. 
  * Most of the forms were originally created by rules, but 
  * a later generation never grasped the rules and instead memorized the forms as words.

---

### Suffix Irregularity

The present progressive suffix _-ing_, as in _The joint is jumping_, is 100 percent regular. 
  * There isn’t a single exception to the rule, not even the rebellious _be_, which meekly submits and shows up as _being_. 
  * Why, when it comes to _-ing_, does no verb hear a different drummer? 
    * One reason is that the progressive construction came into English relatively recently, late in the Middle English period of 1100 to 1450. 
    * It borrowed the _-ing_ suffix from the gerund (a construction that turns a verb into a noun, as in _the changing of the guard_), and the newly cloned _-ing_ suffix had the progressive all to itself and did not have to compete with alternative forms.

One other suffix is completely regular: the possessive _s_. 
  * Any noun can take it, even the irregular nouns that cannot appear with an `/s/` sound when it is a plural suffix, such as _mouse_ and _man_.
  * The possessive is unusual because it attaches to a phrase rather than to a word. 
    * One can talk not just about the _cat’s pajamas_ but about the _cat in the hat’s pajamas_, where the pajamas belong to the cat, not to the hat.
    * The exceptions that prove the rule are the possessive pronouns _my, your, his, her, our,_ and _their_, which are, in a sense, irregular replacements for _me’s, you’s, him’s, her’s, us’s,_ and _them’s_. Pronouns are one-word phrases; in any sentence position where you can say _the man in the gray suit_ you can also say _he_ or _him_.

The third-person singular _-s_, as in _Dog bites man_, steps aside for irregular forms in only four verbs.
  * _be—is_ (not _be’s_), _have—has, do—does_ (pronounced `/dŭz/`), and _say—says_ (pronounced `/sĕz/`). 
  * These, by the way, are the four most frequent verbs in the English language.

---

### Digression: Irregular Plurals

Of the count nouns, which do take plurals, exactly seven change their vowel instead of adding _-s_: _man-men, woman—women_ (pronounced wĭmɨn), _foot—feet, goose—geese, tooth—teeth, mouse—mice, louse—lice_

Another three irregular plurals take the old Anglo-Saxon suffix _-en_ rather than _-s_: _child—children, ox—oxen, brother—brethren_

Several names for gregarious animals that are hunted, gathered, or farmed are identical in the singular and plural: _fish, cod, flounder, herring, salmon, shrimp deer, sheep, swine, antelope, bison, elk, moose, grouse, quail._

A fourth class of nouns takes the regular _-s_ ending but changes its final consonant, usually _f_ but sometimes _th_ or _s_, from unvoiced to voiced: _calf—calves_; also _elf, dwarf, half, hoof, knife, leaf, life, loaf, self, scarf, sheaf, shelf, thief, wife, wharf, wolf, mouth—mouths_; also _truth, sheath, wreath, youth, house—houses_

Finally, there are nouns that take Latin or Greek plurals.

Here are five families with Latin plurals: 
1. alumnus—alumni; also bacillus, cactus, focus, fungus, locus, nucleus, radius, stimulus 
2. genus—genera, corpus—corpora 
3. alga—algae; also alumna, antenna, formula, larva, nebula, vertebra 
4. addendum—addenda; also bacterium, curriculum, datum, desideratum, erratum, maximum, medium, memorandum, millennium, moratorium, ovum, referendum, spectrum, stratum, symposium 
5. appendix—appendices; also index, matrix, vortex 

And here are two families with Greek plurals: 
1. analysis—analyses; also axis, diagnosis, ellipsis, hypothesis, parenthesis, synopsis, synthesis, thesis 
2. criterion—criteria; also automaton, ganglion, phenomenon

[These families are] memorized as a list, not the products of a rule attaching _-i_ or _-ae_, because most nouns shun these plurals except in the speech of people with an attitude: 
* apparatus—apparatuses; also bonus, campus, caucus, census, chorus, circus, impetus, prospectus, sinus, status, virus 
* area—areas; also arena, dilemma, diploma, drama, era, etc. 
* album—albums; also aquarium, chrysanthemum, forum, museum, premium, stadium, udtimatum

---

### Irregular Verbs' Histories

_be_: together with _go_, it is the only verb whose past tense is a completely unrelated word, a relation that linguists call suppletion.
  * Suppletion arises from a merger of two verbs. 
  * Old English, spoken from about 400 to 1100, had three verbs for _be_: _beon, esan,_ and _wesan_. 
    * They probably differed in meaning, with beon referring to permanent states and the other bes to temporary ones.
    * Different sets of bes were used in different parts of England.
    * _Beon_ supplied the base form _be_; _esan_ supplied _am, is, and are_; _wesan_ supplied _was_ and _were_.

In the Middle English period the verb _go_ usurped the past-tense form of another verb, _wend_ (as in _to wend one’s way_), namely, _went_. 
* Today the verb _wend_, bereft of its old past-tense form, has the regular past _wended_, but 
* its original form followed a pattern that can be seen today in other irregular verbs, such as _bend—bent, send—sent,_ and _spend—spent_.

Originally [_have_ and _make_] were _haved_ and _maked_, but enough lazy speakers swallowed the consonants that at some point in the Middle English period speakers didn’t hear them and assumed that they were not there at all.

About half the irregulars end in _t_ or _d_, because they originally took some version of the regular _-ed_ suffix but then fell off the regular bandwagon for one reason or another. 
* These lapsed regulars, together with the regulars themselves, were dubbed _weak_, in 1819 by Jacob Grimm because they were too wimpy to [maintain their ending and change their internal vowel via an ablaut.]
* Some version of the weak past-tense suffix _-ed_ can be found in all the Germanic languages, including English, German, Dutch, and the Scandinavian languages.
* Why didn’t [the irregular] weak verbs make life simple and just stay regular? It is because combinatorial rules of grammar have a cost, as we saw in chapter 1: They blindly join things together without looking at what they are made of, and thus can create ungainly chimeras.

[The] no-extra-suffix habit is alive and well in modern speakers. 
  * The psycholinguists who jot down speech errors have found that people are prone to leaving out _-ed_ on regular verbs that end in _t_ or _d_. For example, they say, 
    * _So we test ’em on it_, intending to say _tested_, or 
    * _That’s what I need to do_, intending to say _needed_.
  * When a word has a verbatim replica of a suffix inside it, rather than just a reminder of one, the attempt to add the real suffix often results in clumsiness or unintelligibility.
  * Repeated-suffix phobia is also the explanation for the class that originally contained _wend—went_: _bend-bent_; also _send, spend, lend, rend, build_ bent. 
  * The overeager _-d_ → _-t_ rule can also be blamed for these verbs: _burn—burnt_; also _learn, dwell, spell, smell, spill, spoil_
  * The irregular forms ending in _-t_ show the English language changing before our eyes: Most of them are on their way out.

Starting around the year 1000, English speakers shortened their pronunciation of a vowel when extra phonetic stuff (such as a consonant or syllable) was added, pushing new consonants into the syllable.  
* Here are some examples that have survived in modern English: 
  * bone—bonfire 
  * break—breakfast 
  * child—children 
  * Christ—Christmas 
  * deep—depth 
  * five—fifth 
  * know—knowledge 
  * sheep—shepherd 
  * wide—width 
  * wise—wisdom 
* Shortening a vowel is a natural reaction when material is added to the end of the syllable. A syllable is a unit of timing, taking up a constant tick of the speech clock. If material is added to the end of a syllable, the vowel is often shortened to maintain the rhythm.
* Modern irregular past-tense forms that would be regular but for their shortened vowels: _keep—kept_; also _creep, leap, sleep, sweep, weep_

Add some other habits of Middle English speakers that we have come across -- using _-t_ more widely, dropping suffixes -- and you understand many other irregular verbs in modern English: 
* _feel—felt_; also _deal, kneel, dream, leave_ 
* _bleed—bled_; also _breed, feed, lead, mislead, plead, read, speed, meet_ 
* _hide—hid_; also _slide, bite, light, alight flee—fled, say—said, hear—heard, lose—lost, shoot—shot_ 
* _sell—sold_; also _tell, foretell_

The pairs of vowels traditionally called “long” and “short,” and spelled as if they are double and single scoops of the same sound, are in fact very different vowels. How did that happen?
* For many centuries speakers of Old and Middle English enhanced the difference between short and long vowels by making the long vowels tense: 
* that is, the muscle at the root of the tongue is tensed up, changing its shape and making the vowel in _great_ sound different, as well as longer, than the vowel in _get_. 
* Enhancement went wild, however, during the dawn of Early Modern English in the fifteenth century, when the pronunciation of the long vowels was scrambled in a linguistic revolution called the Great Vowel Shift. 
  * Before the shift, _keep_ had been pronounced something like _cape_, _hide_ like _heed_, _boot_ like _boat_. 
  * After the shift, the English spelling of the long vowels no longer made much sense, nor did the pairings of “short” and “long” vowels in siblings like _keep_ and _kept_. 
* Since the children of Early Modern English could not have heard a relationship between the vowels, the past-tense forms struck them as a ragbag that just had to be memorized outright, and so they remained for subsequent generations. 
* Thus [some] verbs [did not undergo a vowel change, such as those]...
  * that entered the popular language after the Great Vowel Shift, such as _peep_ (1460) and _seep_ (1790), and 
  * verbs whose pronunciations eventually drifted into rhyming with the _keep_ verbs, such as _reap_ and _heap_
  * they remained intact when they first submitted to _-ed_, giving us _peeped, seeped, reaped,_ and _heaped,_ not _pept, sept, reapt,_ and _heapt_.

According to the theory that irregulars are pairs of memorized words, an irregular past-tense form could, in principle, survive in memory without a corresponding stem. 
  * _Wrought_ appears to be an example: Most people have no idea what the verb is.
    * Many guess _wreak_ (based on an analogy with _seek—sought_) or _wring_ (based on an analogy with _bring—brought_), but both guesses are wrong. The answer is _work_.
  * _Wrought_ belongs to a family of verbs that replace their rhyming parts with _ought_ or _aught_: _buy—bought_; also _beseech, bring, catch, fight, seek, teach, think_
  * A vowel and an adjacent _r_ often switched places in the history of English, 
    * because _r_ sounds a lot like a vowel, which makes its order with respect to a vowel hard to hear. 
    * Thus _brid_ became _bird_, _thrid_ became _third_, _hross_ became _horse_, and _worght_ became _wroght_.
  
Proto-Indo-European apparently had a set of rules for forming the past tense, not by adding a suffix as in modern English, but by changing the vowels, as in modern Hebrew—a kind of rule called gradation, apophony, or ablaut. 

The verb _werg-_ (to do) ended up... 
  * in Germanic as _werkam_ (work), and 
  * in Greek as _erg-_ (action) and _org-_ (tool), which eventually crossed over 
  * into English as _energy_, _organ_, and _orgy_. 

Sometimes strong and weak forms live side by side in a person’s mind, forming doublets like _strove_ and _strived_ or _dove_ and _dived_.
* Doublets usually arise when an irregular form (such as _strove_) hovers in a twilight zone in memory and people are not sure whether they have heard the form or are confusing it with a similar form, like _drove_. 
* Other doublets arise [because] Britain and America are divided by a common language. The British prefer _dived_, the Americans prefer _dove_...
* Often the members of a doublet will diverge in meaning, grammar, or formality, like twins who strive not to be confused. 
* _Shone_, for example, is intransitive (without a direct object), as in _The stars shone in the sky_, and a touch poetic, whereas _shined_ is an everyday form that may be used in transitive sentences such as _Melvin shined his shoes_.
* For many people regular _hanged_ means _“suspended by the neck until dead,”_ irregular _hung_ merely _“suspended._”

The high-front vowels come first in expressions such as _pitter-patter_ and _dribs and drabs_; we don’t say _patter-pitter_ or _drabs and dribs_. And in pairs such as _this and that_, _here and there_, and _me and you_, the higher and farther-to-the-front vowels are found in the word that means “self” or “near the self,” the lower and farther-to-the-back word means “other” or “far from the self.” That is true not only in English but in many families of languages...
  * The linguist Roger Wescott has pointed out that high front vowels are pronounced with a constricted mouth cavity and the tongue close to the visible part of the vocal tract, whereas low back and central vowels are pronounced with a large mouth cavity and the tongue buried from view. 
  * That may call to mind the conceptual distinction between presentness and pastness. 
    * Pastness may remind people of a cavity or space, because a past event is separated by an interval of time from the present moment, and metaphorically speaking time equals space. 
    * It may also remind people of remoteness or distance, because metaphorically speaking long ago equals far away. 
  * Perhaps as Indo-European was developing, speakers vaguely felt that lower and farther back vowels fit better with the concept of an event separated in time from the present, and that higher and farther front vowels fit better with an event in the here and now.
  * This contrast, between a higher front vowel and a lower back vowel, survives in the majority of modern English irregular verbs. The base forms have sounds like _ĕ_ and _ē_ and _ĭ_ and _ī_ and _ā_, and the past-tense forms have sounds like _ă_ and _ŏ_ and _ō_ and _ŭ_.
  * _Ring—rang_ originally was weak (with a past tense like _ringed_) and was attracted to the _ing—ang—ung_ class by analogy to verbs like _sing_. The same happened to _dig—dug, stick—stuck, wear—wore, show—shown,_ and many others.
  * _Kneel—knelt, dive—dove, catch—caught, and quit—quit_ became popular only in the nineteenth century; George Washington, for example, used _catched_, and Jane Austen used _quitted_.

---

### Words vs Rules: Generative Phonology

1. According to the theory of generative phonology developed by Noam Chomsky and Morris Halle, rules rule.
  * Every drop of patterning in past-tense forms, regular or irregular, is squeezed out into rules, and 
  * only the compressed, desiccated residue is stored in the mental lexicon. 
2. According to the theory of parallel distributed processing or connectionism developed by David Rumelhart and James McClelland, there are no rules: 
  * People store associations between the sounds of stems and the sounds of past-tense forms, and 
  * generalize the associations to new words if they are similar to old words.

Irregular past-tense forms are similar in sound to their base forms. Let’s call this pattern **stem-past similarity.** 
* For example, drink and drank share _d, r,_ a vowel, _n,_ and _k_; the only difference is that drank has the vowel a where drink has the vowel _i_.
* In fact, all the irregulars except _go—went_ and _be—was_ share material with their stems.

Second, a few kinds of change from a stem to its past are seen over and over among the 164 irregular verbs. Let’s call this pattern, in which the change from stem to past in one verb is similar to the change from stem to past in another verb, **change-change similarity.**
* The _ĭ-ă-ŭ_ pattern in _drink—drank—drunk,_ for example, is found, with variations, in twenty other verbs. 
* Similarly, we have _freeze—froze, speak—spoke,_ [and others].

Third, the verbs undergoing a given irregular change are far more similar than they have to be. Let’s call this pattern **stem-stem similarity.**
* If you are a verb and want to undergo the _ĭ-ă-ŭ_ pattern, all you really need is an _ĭ_. 
* But the verbs that do follow the pattern (_drink, spring, shrink,_ and so on) have much more in common; most begin with a consonant cluster like _st-, str-, dr-, sl-, or cl-,_ and most end in _-ng_ or _-nk_. 
* Similarly, the verbs whose pasts end in _-ew_ (_blow, grow, throw, slay, draw,_ and _fly_) tend to begin with a consonant cluster and end with a vowel.

The trick is to find a compact set of rules that captures the generalizations the mind likes to make. By far the most ambitious theory of this kind comes from Noam Chomsky and Morris Halle’s 1968 magnum opus _The Sound Pattern of English_.
  * Chomsky, Halle, and Mohanan accounted for the patterns of thousands of English words with just a few dozen phonological rules... Their theory comes from a field called **generative phonology.**
  * Chomsky, Halle, and Mohanan roundly reject the words-and-rules dichotomy. Verbs sit on “a continuum of productivity and generality that extends from affixation of the _-ed_ suffix in _decide—decided_ to total suppletion in _go—went_.”

A monumental contribution of generative phonology was to slice rules even more finely so that they apply not to vowels and consonants but to the components of vowels and consonants, called features.
  * One rule can state, “At the end of a word, insert _i_ to separate adjacent consonants that have similar features for place and manner of articulation” —- no listing of _t, d, s, z, -ed,_ or _-s_ is necessary.
  * Similarly, it’s obtuse to have a rule that puts a _t_ after consonants in the list _p, k, f, s, sh, ch,_ and _th_ -- all these consonants obstruct the air stream (they are obstruent) and all are unvoiced. The rule can simply say, “At the end of a syllable, copy the voicing feature from one obstruent consonant to the next.”
  * A rule that simply said, “For the following irregular verbs, lower the vowel” would have to tinker with only one feature, not five... This simple rule, **Lowering Ablaut**, is one of the three irregular rules that Halle and Mohanan’s theory gets away with. 
  * The other two are **Backing Ablaut**, which replaces mid-front-e in _bear_ with mid-back-o in _bore_, and 
  * **Shortening Ablaut**, which replaces the long vowels in _flee_ and _shoot_ with their short counterparts in _fled_ and _shot_.

As we saw in the last chapter, “long” and “short” have been misnomers in English at least since the Great Vowel Shift in the fifteenth century, when people scrambled the pronunciations of vowels. 
  * Those vowels are not particularly similar, and a rule capable of replacing one with another is capable of doing almost anything. 
  * To say that a Shorten-the-Vowel rule simplifies the hairy irregular verbs sounds like a hoax.

In the case of the so-called long vowels in English, Chomsky and Halle proposed that the underlying forms really are long versions of the short vowels. 
  * That is, in the mental lexicon the vowels in the following pairs are identical in every respect except how long it would take to pronounce them.
  * In their actual pronunciations these pairs do differ in how long it takes to say the vowel, but they differ in many other ways besides, so why assume that the mind lists only the difference in length? 
  * The reason, according to Chomsky and Halle, is that the other differences are redundant and predictable, hence unnecessary to list. No pair of English words differs only by vowel length. 
    * Long vowels are also tense and diphthongs (they glide to a different vowel at the end); 
    * short vowels are lax and not diphthongs.
  * Chomsky and Halle therefore proposed that English has a rule of 
    * **Long Vowel Tensing**, which tenses all long vowels, and a rule of 
    * **Diphthongization**, which adds the little _y_'s and _w_'s [to vowels].
  * All this is more or less unexceptionable -— something in the mind of an English speaker enforces a correlation among length, tenseness, and being a diphthong, and the predictable details of pronunciation need not be stored in individual lexical entries.
  * [As a corrolary example] The _nd_ is pronounced as _nj_ in _hand you_, as _m_ in _hand me_, and as the _ng_ sound in _hand care_. 
    * But if, as Chomsky and Halle proposed, the dictionary entries for words are schematic -- so that 
      * the last segments of _hand_ are listed, say, as “nasal” and “dental” rather than as _n_ and _d_, and 
      * they are fleshed out into full consonants by rules that work in different ways in different contexts -- then 
    * a single representation could embrace the _hand_ that appears in _hand, hand me, hand you,_ and _hand care_.

One problem comes from the assumption that every scintilla of patterning in the verb system needs an explanation in terms of the psychology of speakers, in particular that the patterns are distilled out into rules in the mind. 
* Chomsky, Halle, and Mohanan’s rule-by-rule derivations often recapitulate the history of a past-tense form in English over the centuries -— deliberately -- and that brings to mind an alternative explanation... that the patterns are fossils of rules that died long ago. 
* The surviving past-tense forms, semilawful though they are, could simply be memorized by today’s generation without any help from the rules. 

The defunct-rule explanation has an advantage over the Chomsky-Halle-Mohanan theory.
* If the rules and underlying forms are to play some role in mental life, children must infer the cascade of rules that generated the surface form, run it in reverse, and extract the underlying form. 
* And the suggestion that English-speaking children hear _run_ and infer _rin_ or hear _fight_ and infer the German-sounding _fēcht_ is, frankly, beyond belief.
  * Chomsky, Halle, and Mohanan proposed that the _ch_ of _Bach_ is a covert English phoneme that lives underground in the lexical entries for _buy_ and _fight_ -— namely, _bēch_ and _fēcht_ -— and in the half-baked past-tense forms for _seek_ and _teach_. 
  * Of course the _ch_'s must be assassinated before they see the light of day, but not until they have triggered a rule that makes the past-tense form come out right. 
    * _Bēch_ gets a _-t_, 
    * changes to _bôcht_ by the Lowering and Backing Ablaut rules, at which point 
    * the _cht_ triggers Cluster Shortening to yield _bŏcht_ before 
    * _ch_ makes the ultimate sacrifice, 
    * resulting in the form we spell _bought_.
  * Why would the child bother if the rules are there only to generate the surface form, and the child already has the surface form? (It’s different with sentences, where the child needs rules to generate an infinite number of new ones; with word roots, there are only a finite number to learn.)
* At one point Chomsky and Halle concede the problem and say that their grammar is only what children would construct if, hypothetically, they could hear the entire vocabulary in one sitting before figuring out the rules, rather than learning the everyday words first. 
* But then it’s not clear what their theory is a theory of—it is not, by their concession, a theory of how real children acquire words or how real adults represent them.

The biggest problem is that the Chomsky-Halle-Mohanan theory cannot explain the third kind of similarity running through the irregulars: the similarities among stems, as in _sting, string, sling, stink, sink, swing,_ and _spring._
  * The list of verbs assigned to the rule could just as easily have been _till—tull, wish-wush, fib-fub,_ and _pith—puth_. 
  * How can a theory that relentlessly soaks up 
    * every droplet of redundancy between stems and pasts, and 
    * between the changes applying to one stem and the changes applying to another stem, 
  * be so oblivious to the massive redundancy among all the stems undergoing a change?

The problem, first pointed out by the linguist Joan Bybee and the psychologist Dan Slobin, is that the irregular clusters are family resemblance categories. 
  * They don’t have strict, all-or-none definitions that specify which verbs are in and which verbs are out. 
  * Instead they have fuzzy boundaries and members that are in or out to various degrees depending on how many properties they share with one another.
    * _Spin_ and _stick_ each misses by a different feature; 
    * _dig—dug_ and _win—won_ are farther toward the periphery; and 
    * _sneak—snuck, drag—drug, skin—skun,_ and _climb—clumb_ are in a muzzy zone at the edge where speakers differ as to their acceptability. 
    * [Thus] No rule can cleanly pick out the _ĭ-ŭ_ verbs.
  * The other irregular families work in the same way.

Bybee and Moder even quantified the effect by presenting their adult volunteers with nonsense words that varied in similarity to the typical members of the _ing—ung_ family. 
  * _Spling_ and _skring_ fall smack in the middle of the family, and about 80 percent of the participants came up with forms like _splang, splung, skrang,_ and _skrung_. 
  * _Krink, trig,_ and _pling_ are less similar, and only about 50 percent of the people suggested _krunk, trug,_ or _plang_. 
  * _Vin, sid,_ and _kib_ share only a vowel with the verbs in the family, and only about 20 percent of the people provided forms like _vun, sud,_ or _kub_.

Chomsky, Halle, and Mohanan have tweaked rules for maximum performance, but at a steep price. They were forced to make incredible claims about the mental entries of words, and their theory cannot handle the fuzzy and statistical -- but psychologically active -- patterns of similarity among the verbs undergoing a rule.

---

### Words vs Rules: Connectionism

When the psychologists David Rumelhart and James McClelland announced their artificial neural network model of the past tense in 1986, the reaction was sensational. 
* Here was a model with none of the paraphernalia of linguistics -- no words, no rules, no modules -- but it... 
  * acquired several hundred regular and irregular past-tense forms, 
  * generalized their patterns to new verbs, and 
  * made errors such as _breaked_ and _comed_, just like children.
* Rumelhart and McClelland’s model helped to launch a new school of cognitive science known as **connectionism** or parallel distributed processing, which explains mental processes in terms of networks of interconnected simple units that vaguely resemble neurons (brain cells).

The difference between connectionism and generative grammar lies in the kinds of mental operations that are thought to be implemented in neural networks. 
  * In particular, connectionism differs from generative grammar in the way that associationism differs from symbol manipulation. 
  * It lacks combinatorial rules organized into modules, and instead tries to accomplish intelligence using... 
    * Hume’s **law of contiguity** (if A appears with B, associate them) and 
    * his **law of resemblance** (if C looks like A, let it share A’s associations). 
  * A **neural network** that works this way is called a pattern associator memory or a perceptron.

The input to the model is the sound of a verb stem, and the past tense is computed from it.
  * As with Chomsky and Halle, a single kind of machinery is charged with computing the past-tense forms of all verbs, regular, irregular, and suppletive (_go—went_); the verbs sit on a continuum of regularity from completely predictable to completely arbitrary. 
  * Past-tense forms are composed piecemeal out of miniregularities that are shared among verbs, so that _sleep—slept_ combines 
    * the vowel change in _feed—fed_ and 
    * the suffixation in _burn—burnt_. 
  * Rumelhart and McClelland also import the standard Chomskyan assumption that speech sounds are represented in the mind not as phonemes but as bundles of features such as “voiced” and “nasal.”

The left-hand column is the input layer, where the verb stem is entered. 
  * It contains 460 vaguely neuronlike units, each of which can be either on or off. 
  * Each unit represents a tiny stretch of sound that might appear in an English verb, such as 
    * a high vowel between two stop consonants, or 
    * a back vowel followed by nasal consonant at the end of the word.
  * There are no units for individual verbs; a verb is entered by turning on the units for the sounds it contains. 
  * As a result, similar-sounding verbs share representational real estate. 
  * Most of the units that are turned on when _shrink_ is fed in are also turned on when _drink_ is fed in (consonant cluster at the beginning of the word, high vowel between two sonorant consonants, and so on).

The right-hand column has an identical bank of units, and they represent the output of the model: the sound of the past-tense form. 
  * Every input is connected to every output by a synapselike connection that can vary in strength, from 
    * strongly excitatory (an input signal tends to turn the unit on), to 
    * neutral (an input signal has no effect), to 
    * strongly inhibitory (an input signal tends to turn the unit off). 
  * In effect each connection is a probabilistic microrule that states something like, 
    * “If the stem contains a stop consonant followed by a high vowel, 
    * [then] the past-tense form is likely to contain a nasal consonant at the end.” 
  * With 460 input units connected to 460 output units, we have 460 x 460 = 211,600 microrules in all. 
  * When an input unit is turned on, 
    * it sends a signal down all its lines to the output layer, where 
    * the signal is multiplied by the strength of each connection and 
    * fed to that output unit. 
  * Whether a given output unit turns on depends, in a probabilistic way, on 
    * the sum of the signals that feed into it and on 
    * its own level of triggerhappiness or threshold. 
  * The higher the summed signal is above the threshold, the more likely the unit is to turn on; 
  * the lower the summed signal is below the threshold, the more likely the unit is to turn off. 
  * In the neonate network the connections have strengths of zero, so the output layer is completely off, regardless of the input. 
  * The connections then are changed in a learning procedure, in which the model is “taught” with a set of verbs and their correct past-tense forms.
  * A given connection will be buffeted up and down by successive verbs in a training run, but eventually it will settle on the strength value that does the best job, in combination with the other connections, of producing correct past-tense forms. 
  * The network’s knowledge of the various verbs and their past-tense forms is smeared across the 211,600 connection strengths; 
  * one cannot point to a circumscribed part of the network that implements a particular word, a particular irregular family, or a regular rule.

Rather than associating a word with a word, it associates the properties of a word -- its phonological features -- with the properties of another word, and thereby enjoys automatic generalization by similarity. 
  * That is, rather than associating _drink_ with _drank_, 
  * it associates _dr_ with _dr_, _dr_ with _rang_, _ring_ with _rang_, _ink_ with _ank_, and so on. 
  * At the same time, it negatively associates _dr_ with _nked_, _ink_ with _nked_, and so on, inhibiting the incorrect regular form _drinked_. 
  * Crucially, these associations are superimposed across the different words in the training set. 
    * When the model trained on _drink_ is then trained on _shrink_, it strengthens many of the same connections, such as _ring_ with _rang_ and _ink_ with _ank_. 
    * That makes shrink easier to learn -- most of its connections have been prestrengthened -- and it makes subsequent family members, such as _sink_, easier still.

The theory is that:
1. Children, when hearing a past-tense form in their parents’ speech, 
  * recognize that it is the past-tense form of a familiar verb, 
  * dredge the verb stem out of memory, 
  * feed it into their past tense network, and silently 
  * compare their network’s output with what they just heard.
2. The correct form from the parents is displayed in a special layer of “teacher” units. 
  * The model compares its output, unit by unit, with the correct output (walked for walk, came for come, and so on). 
  * The model then adjusts the connection strengths a tiny amount up or down depending on the difference.

[One divergence from human neural networks is that] Rumelhart and McClelland’s pattern associator memory is a device that only produces past-tense forms. 
* You cannot turn the arrows around and get the model to run backward and recognize past-tense forms. 
* Obviously people do both.

Second, the model computes every detail of the pronunciation of the past-tense form. 
  * Yet we saw that many of these details, such as the choice among _-t, -d,_ and _-id_, are found in fifteen different parts of the language system. 
  * Surely they are computed by a single phonology module that is fed by the output of morphology and syntax, 
  * not duplicated by an amazing coincidence in fifteen different networks, one for the past tense, one for plurals, and so on.

Third... the model cannot tell the difference between two words that have the same sound. 
* [By forgoing the use of lexical entries and relying entirely on a word’s sound to compute its past-tense form] it must give [the two words] the same past-tense form, and 
* that won’t work for soundalike verbs like _ring—rang_ and _wring—wrung_, _break—broke_ and _brake—braked_, or _meet—met_ and _mete—meted_.

[Fourth, ] the model was mute when asked for the past tenses of simple but somewhat unusual-sounding words, like _jump, pump, warm,_ and _trail_. And it garbled several others, turning _squat_ into _squakt_, _tour_ into _toureder_, and _mail_ into _membled_.

The lapses are puzzling to us because intuitively nothing could be simpler than copying a stem over to the past-tense form before adding _-ed_. 
  * But a pattern associator memory has no placeholder called “stem” that can be copied, and no operation to do the copying. 
  * All it does is associate sounds with sounds, and 
  * if the training set happens to be missing words with certain combinations of sounds such as _-ump_ or _-ail_, the model will be at a loss...

Symbolic trees require fancier neural hardware than the smooth purée of units that are popular among connectionists, but those models hardly do justice to the brain anyway. 
* Recently, a few neural network modelers have shown how hierarchical trees can be implemented in more organized neural networks. 
* One conjecture is that the periodic rhythms of neural firing, long downplayed in neuroscience, serve as the glue that binds together 
  * the units that represent an abstract slot in a tree and 
  * the units that represent its content. 
* For example:
  * when the units for the “coda” slot fire at twenty times a second, and 
  * the units for _p_ fire in synchrony with them, also at twenty times a second, 
  * the system as a whole knows that the coda is _p_. 
  * Simultaneously, the units for “nucleus” can be firing thirty times a second, and 
  * the units for _i_ can be firing in synchrony thirty times a second, and the system knows that the nucleus is _i_.

### Words vs Rules: Mu

One phenomenon, two models, both explaining too much to be completely wrong, both too flawed to be completely right. 

Prince and I have proposed a hybrid in which:
1. Chomsky and Halle are basically right about regular inflection and 
2. Rumelhart and McClelland are basically right about irregular inflection. 

Our proposal is simply the traditional words-and-rules theory with a twist. 
* Regular verbs are computed by a rule that combines a symbol for a verb stem with a symbol for the suffix. 
* Irregular verbs are pairs of words retrieved from the mental dictionary, a part of memory. 
* Here is the twist: Memory is not a list of unrelated slots, like RAM in a computer, but is associative, a bit like the Rumelhart-McClelland pattern associator memory. 
  * Not only are words linked to words, but bits of words are linked to bits of words. 
  * The bits are not Wickelphones, of course, but substructures like stems, onsets, rimes, vowels, consonants, and features,
  * Furthermore, the nodes of one word (such as _string_) overlap the same nodes in other words (such as _sling, stick, stink,_ and _swim_). 
  * As a result, irregular verbs show the kinds of associative effects found in a connectionist pattern associator.
    * People find families of similar irregular verbs easier to store and recall because these verbs repeatedly strengthen their shared associations. And 
    * people occasionally generalize the irregular patterns to new, similar verbs, because the new verbs contain material that already had been associated with the pattern from the old verbs.

[Mark Aronoff](https://linguistics.stonybrook.edu/people/_bios/_linguistics-faculty/mark-aronoff.php), [Joan Bresnan](https://web.stanford.edu/~bresnan/bio/index.html)... and others have suggested that language uses two kinds of rules: 
1. true rules that speakers generalize freely, and 
2. lexical redundancy rides that merely capture patterns of similarity among words stored in memory.

The prediction is that regular and irregular inflection are psychologically, and ultimately, neurologically distinguishable.
* Irregular inflection depends on memorized words or forms similar to them, but 
* regular inflection can apply to any word, regardless of whether the word is readily retrievable from memory. 
  * Regular inflection has that power because it is computed by a mental operation that does not need access to the contents of memory, 
  * namely, a symbol-processing operation or rule, which applies to any instance of the symbol “verb.”

If the modified words-and-rules theory is correct, it would have a pleasing implication for the centuries-old debate between associationism and rationalism: Both theories are right, but they are right about different parts of the mind.
* Chomsky and Halle, in leaching every bit of patterning out of memory and concentrating it in rules, 
  * had to propose implausible deep structures for words, and 
  * could not explain why irregular verbs come in families of similar forms. 
* Rumelhart and McClelland, in dismantling every bit of structure in language, 
  * had to propose clumsy Wickelphones to represent words, and 
  * could not explain why regular verbs are copied so reliably into their past-tense forms.

The evidence for this hybrid model, we shall see, is that 
* when people use an irregular form, they must have that form or similar forms in memory, whereas 
* when they use a regular form, they don’t need to access memory at all.

---

### Irregular Verb Loss

The words-and-rules theory predicts that rarity should hurt an irregular verb, but not a regular verb.
* Irregular verbs are the most common verbs and vice-versa, in English and in most other languages. The explanation is simple. 
  * Irregular forms have to be memorized repeatedly, generation after generation, to survive in a language, and the commonly heard forms are the easiest to memorize. 
  * If an irregular verb slips in popularity, a generation of children will fail to hear its past tense often enough to remember it.
* [Joan Bybee](http://www.unm.edu/~jbybee/) did some historical digging to prove this conjecture. 
  * Remember that Old English had about three times as many strong irregular verbs as Modern English...
  * Bybee looked at thirty-three strong verbs that survived in Modern English, and divided them into verbs that remained irregular and verbs that became regular. 
  * She then looked up their modern frequencies in the Brown corpus. 
  * The still-irregular verbs appear an average of 515 times per million words; the regular defectors appear an average of 21 times.

When you produce an irregular form, you not only have to dredge it out of memory but also must repress the “Add -ed” rule so you don’t say _breaked_ or _broked_.  
  * Linguists call this principle **blocking** -- the irregular form blocks the rule.
  * Words and rules are accessed in parallel... As we plan to utter a verb in the past tense, we simultaneously look up the word in memory and activate the rule. 
  * An inhibitory link runs from the memory box to the rule box, which gradually slows down the rule as evidence for a match is found, and eventually turns it off.
  * If a memory entry is faint or blurry because the word is uncommon, the matching and fetching will be especially erratic, and often the rule may not be braked in time.

---

### Aristotle, Wittgenstein, and Categories

A “turtle” is a reptile with a broad flattened body enclosed in a shell formed of a dorsal carapace and a ventral plastron, united at the sides. 
* The power of a definition is that it transcends the particulars of experience. 
* People can recognize a new turtle when they see one, as long as it conforms to the definition. 
* Psychologists call these categories **“classical” or “Aristotelian” categories**, after the Greek philosopher who emphasized logic and definitions as the basis of knowledge. 
* For decades psychologists studied concept learning in humans and animals by presenting them with drawings of colored shapes, indicating which ones belonged to a category such as “large red square,” and measuring how long it took the subjects to infer the category.

Wittgenstein... noted [that] a category can be extended to embrace new cases “_as in spinning a thread we twist fibre on fibre. And the strength of the thread does not reside in the fact that some one fibre runs through its whole length, but in the overlapping of many fibres._” In the 1970s the psychologist Eleanor Rosch brought Wittgenstein’s ideas into psychology by showing that many human concepts picked out **family resemblance categories** rather than classical categories.
1. First, with most categories it is almost impossible to find a set of membership conditions.
1. Second, the members of a category are not created equal, which is what one would expect if they were admitted into the category by meeting the definition.
  * The best member of all is called the prototype, and it sums up the category in people’s minds. 
  * Dictionaries often show a prototype next to the definition of a category.
  * [Examples include] the sparrow for “bird” and a wrench for “tool”...
1. Third, the categories of the mind have fuzzy borders. 
  * People aren’t quite sure whether garlic, parsley, seaweed, or edible flowers should count as vegetables...
1. Fourth, most of our everyday categories, and not just games, show Wittgenstein’s family resemblance and crisscrossing features. 
  * Many vegetables are green, but carrots aren’t; many are crunchy when raw, but spinach isn’t.
1. Fifth, categories have stereotyped features: traits that everyone associates with the category, even if they have nothing to do with the criteria for membership. 
  * When people think of a grandmother, they think of gray hair and chicken soup, not of a node in a genealogical tree.

Many experiments have confirmed that everyday concepts act like family resemblance categories.
* People are comfortable with the very idea that categories have a better and worse members... 
* It’s also easy to show in the lab that people are fuzzy about borderline cases.

Does this mean that people’s heads are stuffed with fuzz and that classical categories are fictions? Surely not. 
* People can learn categories with clean definitions, crisp edges, and no family resemblance, such as “odd number.” 
* They can learn that a dolphin is not a fish, though it has a strong family resemblance to the fishes, and that a seahorse is a fish, though it looks more like a little horse.

Family resemblance categories are real, but so are classical categories; they live side-by-side in people’s minds, as two ways of construing the world.
* Clusters of irregular verbs pass all five tests Wittgenstein’s family resemblance categories... [while] the regular verbs... pass all the test of classical categories.
* Why on earth should irregular verbs act like games and furniture and vegetables, and regular verbs act like grandmothers and odd numbers? 

The facts about verbs and the facts about concepts converge to suggest that the human mind is a hybrid system, learning fuzzy associations and crisp rules in different subsystems. Some modelers even link:
* the rule system to the frontal cortex and 
* the exemplar-based system to the temporal and posterior cortex, 
* much as we did for rules and words...

...people form categories that give them an advantage in reasoning about the world by allowing them to make good predictions about aspects of an object they have not directly seen. 
* We have to observe a few traits that the object wears on its sleeve and infer the traits that we cannot see directly. 
  * Good categories let us do that. If Tweety has feathers and a beak, Tweety is a bird; if Tweety is a bird, Tweety is warm-blooded, can fly, and has hollow bones. 
  * Bad categories do not: if we only know that Tweety’s name begins with a “T,” nothing of interest would follow.
* These inferences work only if the world is properly structured. Luckily for us... We live in a lawful world in which traits tend to hang together in the same way in many objects.
* But often mortal knowers have no choice but to use [induction]... Many things we find around us could not be deduced by any body of laws, because they are shaped by myriad events of history no longer visible to us.
  * Take birds.... if we could go back and look at [their] last common ancestor... they would be as similar as a single species is today. 
  * But that uniformity did not last for long. Their descendants went off in different directions... 
  * The aftermath of this radiation is a family resemblance category.
* Not all family resemblance categories start off in lockstep and then diversify, but probably all of them are governed by 
  * hidden laws that make them similar and 
  * historical contingencies that make them different.
* If we evolved a taste for family resemblance categories because they really do exist in the world as a product of history, why did we also evolve a taste for classical categories? 
  * I think it is because classical categories are byproducts of rules in the mind that allow us to exploit laws in the world.
  * Classical categories are not free-floating definitions... Odd numbers belong to arithmetic... grandmothers to kinship, dolphins to biological taxonomy... 
  * Each system allows a person to deduce unobservable traits from observable ones, not by remembering that they co-occurred but by cranking through a chain of implications.
  * Since these rule systems are... combinatorial and recursive, they allow us to reason about an unlimited range of cases, often far from our experience.

We have seen that much of the richness of language comes from the tension between words and rules. In the same way, much of the richness of the public sphere of life comes from tensions between family resemblance categories build from experience in the classical categories defined by science, law, or custom.
* Family resemblance categories resonate with common sense, but leave us groping when faced with something that is neither fish nor fowl. 
* Classical categories offer neat divisions, but are bound to seem legalistic, pedantic, or abstruse.
* [In] an analog world... a part of our minds is digital. 
  * We remember familiar entities and they are graded, crisscrossing traits, but 
  * we also generate novel mental products by reckoning with rules. 
* It is surely no coincidence that the species that invented numbers, ranks, kinship terms, life stages, legal and illegal acts, and scientific theories also invented grammatical sentences and regular past-tense forms.